2025-12-16 16:55:20,633 - INFO - ================================================================================
2025-12-16 16:55:20,633 - INFO - GPU EMOTION DETECTION EXPERIMENTS
2025-12-16 16:55:20,633 - INFO - ================================================================================
2025-12-16 16:55:20,633 - INFO - TensorFlow version: 2.13.0
2025-12-16 16:55:20,633 - WARNING - From /home/lab/rabanof/projects/Emotion_Detection_DL/run_gpu_experiments.py:445: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.config.list_physical_devices('GPU')` instead.
2025-12-16 16:55:20,686 - INFO - GPU Available: False
2025-12-16 16:55:20,687 - INFO - GPU Devices: []
2025-12-16 16:55:20,687 - INFO - 

Running experiment 1/8: lstm_glove_baseline
2025-12-16 16:55:20,687 - INFO - ================================================================================
2025-12-16 16:55:20,687 - INFO - STARTING EXPERIMENT: lstm_glove_baseline
2025-12-16 16:55:20,687 - INFO - ================================================================================
2025-12-16 16:55:20,687 - INFO - GPUs Available: 0
2025-12-16 16:55:20,687 - INFO - Loading data...
2025-12-16 16:55:20,809 - INFO - Train samples: 16000, Val samples: 2000
2025-12-16 16:55:20,810 - INFO - Preprocessing...
2025-12-16 16:55:24,873 - INFO - Creating GLOVE embeddings...
2025-12-16 16:55:25,372 - INFO - Tokenizer created. Vocabulary size: 15156
2025-12-16 16:55:25,372 - INFO - Loading GloVe embeddings from /home/lab/rabanof/Emotion_Detection_DL/glove/glove.6B.100d.txt...
2025-12-16 16:55:25,373 - ERROR - Experiment lstm_glove_baseline failed: [Errno 2] No such file or directory: '/home/lab/rabanof/Emotion_Detection_DL/glove/glove.6B.100d.txt'
2025-12-16 16:55:25,375 - INFO - 

Running experiment 2/8: gru_glove_baseline
2025-12-16 16:55:25,375 - INFO - ================================================================================
2025-12-16 16:55:25,375 - INFO - STARTING EXPERIMENT: gru_glove_baseline
2025-12-16 16:55:25,375 - INFO - ================================================================================
2025-12-16 16:55:25,375 - INFO - GPUs Available: 0
2025-12-16 16:55:25,375 - INFO - Loading data...
2025-12-16 16:55:25,413 - INFO - Train samples: 16000, Val samples: 2000
2025-12-16 16:55:25,414 - INFO - Preprocessing...
2025-12-16 16:55:29,495 - INFO - Creating GLOVE embeddings...
2025-12-16 16:55:29,992 - INFO - Tokenizer created. Vocabulary size: 15156
2025-12-16 16:55:29,993 - INFO - Loading GloVe embeddings from /home/lab/rabanof/Emotion_Detection_DL/glove/glove.6B.100d.txt...
2025-12-16 16:55:29,993 - ERROR - Experiment gru_glove_baseline failed: [Errno 2] No such file or directory: '/home/lab/rabanof/Emotion_Detection_DL/glove/glove.6B.100d.txt'
2025-12-16 16:55:29,995 - INFO - 

Running experiment 3/8: lstm_word2vec_baseline
2025-12-16 16:55:29,995 - INFO - ================================================================================
2025-12-16 16:55:29,995 - INFO - STARTING EXPERIMENT: lstm_word2vec_baseline
2025-12-16 16:55:29,995 - INFO - ================================================================================
2025-12-16 16:55:29,995 - INFO - GPUs Available: 0
2025-12-16 16:55:29,995 - INFO - Loading data...
2025-12-16 16:55:30,034 - INFO - Train samples: 16000, Val samples: 2000
2025-12-16 16:55:30,034 - INFO - Preprocessing...
2025-12-16 16:55:34,107 - INFO - Creating WORD2VEC embeddings...
2025-12-16 16:55:34,620 - INFO - Tokenizer created. Vocabulary size: 15156
2025-12-16 16:55:34,621 - INFO - Training Word2Vec embeddings...
2025-12-16 16:55:34,662 - INFO - collecting all words and their counts
2025-12-16 16:55:34,662 - INFO - PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
2025-12-16 16:55:34,718 - INFO - PROGRESS: at sentence #10000, processed 195801 words, keeping 11916 word types
2025-12-16 16:55:34,752 - INFO - collected 15155 word types from a corpus of 311002 raw words and 16000 sentences
2025-12-16 16:55:34,752 - INFO - Creating a fresh vocabulary
2025-12-16 16:55:34,827 - INFO - Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 15155 unique words (100.00% of original 15155, drops 0)', 'datetime': '2025-12-16T16:55:34.827164', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'prepare_vocab'}
2025-12-16 16:55:34,827 - INFO - Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 311002 word corpus (100.00% of original 311002, drops 0)', 'datetime': '2025-12-16T16:55:34.827457', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'prepare_vocab'}
2025-12-16 16:55:34,944 - INFO - deleting the raw counts dictionary of 15155 items
2025-12-16 16:55:34,944 - INFO - sample=0.001 downsamples 48 most-common words
2025-12-16 16:55:34,945 - INFO - Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 210217.32240637334 word corpus (67.6%% of prior 311002)', 'datetime': '2025-12-16T16:55:34.945038', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'prepare_vocab'}
2025-12-16 16:55:35,159 - INFO - estimated required memory for 15155 words and 100 dimensions: 19701500 bytes
2025-12-16 16:55:35,159 - INFO - resetting layer weights
2025-12-16 16:55:35,176 - INFO - Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-12-16T16:55:35.176709', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'build_vocab'}
2025-12-16 16:55:35,176 - INFO - Word2Vec lifecycle event {'msg': 'training model with 4 workers on 15155 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-12-16T16:55:35.176963', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'train'}
2025-12-16 16:55:35,398 - INFO - EPOCH 0: training on 311002 raw words (210091 effective words) took 0.2s, 979518 effective words/s
2025-12-16 16:55:35,574 - INFO - EPOCH 1: training on 311002 raw words (210112 effective words) took 0.2s, 1241256 effective words/s
2025-12-16 16:55:35,749 - INFO - EPOCH 2: training on 311002 raw words (209956 effective words) took 0.2s, 1241921 effective words/s
2025-12-16 16:55:35,925 - INFO - EPOCH 3: training on 311002 raw words (210561 effective words) took 0.2s, 1247713 effective words/s
2025-12-16 16:55:36,100 - INFO - EPOCH 4: training on 311002 raw words (210154 effective words) took 0.2s, 1243115 effective words/s
2025-12-16 16:55:36,100 - INFO - Word2Vec lifecycle event {'msg': 'training on 1555010 raw words (1050874 effective words) took 0.9s, 1137627 effective words/s', 'datetime': '2025-12-16T16:55:36.100866', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'train'}
2025-12-16 16:55:36,101 - INFO - Word2Vec lifecycle event {'params': 'Word2Vec<vocab=15155, vector_size=100, alpha=0.025>', 'datetime': '2025-12-16T16:55:36.101048', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'created'}
2025-12-16 16:55:36,101 - INFO - Word2Vec trained. Vocabulary size: 15155
2025-12-16 16:55:36,249 - INFO - Embedding matrix created. Coverage: 99.99%
2025-12-16 16:55:36,250 - INFO - Converting to sequences...
2025-12-16 16:55:36,764 - INFO - Class weights: {0: 0.5715102157451064, 1: 0.49732686808404825, 2: 2.044989775051125, 3: 1.2351397251814111, 4: 1.3766993632765445, 5: 4.662004662004662}
2025-12-16 16:55:36,764 - INFO - Building model...
2025-12-16 16:55:37,852 - INFO - Model built: LSTM, Parameters: 1,633,722
2025-12-16 16:55:37,852 - INFO - Training LSTM model...
2025-12-16 16:58:39,564 - INFO - Training completed in 181.71 seconds
2025-12-16 16:58:39,564 - INFO - Evaluating...
2025-12-16 16:58:41,112 - INFO - ================================================================================
2025-12-16 16:58:41,112 - INFO - EXPERIMENT COMPLETED: lstm_word2vec_baseline
2025-12-16 16:58:41,112 - INFO -   Validation Accuracy: 0.3435 (34.35%)
2025-12-16 16:58:41,112 - INFO -   Macro F1: 0.0967
2025-12-16 16:58:41,112 - INFO -   Training Time: 181.71s
2025-12-16 16:58:41,112 - INFO - ================================================================================
2025-12-16 16:58:41,118 - INFO - 

Running experiment 4/8: gru_word2vec_baseline
2025-12-16 16:58:41,118 - INFO - ================================================================================
2025-12-16 16:58:41,118 - INFO - STARTING EXPERIMENT: gru_word2vec_baseline
2025-12-16 16:58:41,118 - INFO - ================================================================================
2025-12-16 16:58:41,131 - INFO - GPUs Available: 0
2025-12-16 16:58:41,131 - INFO - Loading data...
2025-12-16 16:58:41,165 - INFO - Train samples: 16000, Val samples: 2000
2025-12-16 16:58:41,165 - INFO - Preprocessing...
2025-12-16 16:58:45,175 - INFO - Creating WORD2VEC embeddings...
2025-12-16 16:58:45,680 - INFO - Tokenizer created. Vocabulary size: 15156
2025-12-16 16:58:45,680 - INFO - Training Word2Vec embeddings...
2025-12-16 16:58:45,715 - INFO - collecting all words and their counts
2025-12-16 16:58:45,715 - INFO - PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
2025-12-16 16:58:45,772 - INFO - PROGRESS: at sentence #10000, processed 195801 words, keeping 11916 word types
2025-12-16 16:58:45,806 - INFO - collected 15155 word types from a corpus of 311002 raw words and 16000 sentences
2025-12-16 16:58:45,806 - INFO - Creating a fresh vocabulary
2025-12-16 16:58:45,882 - INFO - Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 15155 unique words (100.00% of original 15155, drops 0)', 'datetime': '2025-12-16T16:58:45.882379', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'prepare_vocab'}
2025-12-16 16:58:45,882 - INFO - Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 311002 word corpus (100.00% of original 311002, drops 0)', 'datetime': '2025-12-16T16:58:45.882596', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'prepare_vocab'}
2025-12-16 16:58:46,002 - INFO - deleting the raw counts dictionary of 15155 items
2025-12-16 16:58:46,002 - INFO - sample=0.001 downsamples 48 most-common words
2025-12-16 16:58:46,002 - INFO - Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 210217.32240637334 word corpus (67.6%% of prior 311002)', 'datetime': '2025-12-16T16:58:46.002434', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'prepare_vocab'}
2025-12-16 16:58:46,196 - INFO - estimated required memory for 15155 words and 100 dimensions: 19701500 bytes
2025-12-16 16:58:46,196 - INFO - resetting layer weights
2025-12-16 16:58:46,207 - INFO - Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-12-16T16:58:46.207824', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'build_vocab'}
2025-12-16 16:58:46,208 - INFO - Word2Vec lifecycle event {'msg': 'training model with 4 workers on 15155 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-12-16T16:58:46.208117', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'train'}
2025-12-16 16:58:46,387 - INFO - EPOCH 0: training on 311002 raw words (210123 effective words) took 0.2s, 1216347 effective words/s
2025-12-16 16:58:46,563 - INFO - EPOCH 1: training on 311002 raw words (210110 effective words) took 0.2s, 1261535 effective words/s
2025-12-16 16:58:46,738 - INFO - EPOCH 2: training on 311002 raw words (210469 effective words) took 0.2s, 1247174 effective words/s
2025-12-16 16:58:46,919 - INFO - EPOCH 3: training on 311002 raw words (209853 effective words) took 0.2s, 1197543 effective words/s
2025-12-16 16:58:47,094 - INFO - EPOCH 4: training on 311002 raw words (210140 effective words) took 0.2s, 1247661 effective words/s
2025-12-16 16:58:47,095 - INFO - Word2Vec lifecycle event {'msg': 'training on 1555010 raw words (1050695 effective words) took 0.9s, 1184876 effective words/s', 'datetime': '2025-12-16T16:58:47.095046', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'train'}
2025-12-16 16:58:47,095 - INFO - Word2Vec lifecycle event {'params': 'Word2Vec<vocab=15155, vector_size=100, alpha=0.025>', 'datetime': '2025-12-16T16:58:47.095237', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'created'}
2025-12-16 16:58:47,095 - INFO - Word2Vec trained. Vocabulary size: 15155
2025-12-16 16:58:47,248 - INFO - Embedding matrix created. Coverage: 99.99%
2025-12-16 16:58:47,248 - INFO - Converting to sequences...
2025-12-16 16:58:47,888 - INFO - Class weights: {0: 0.5715102157451064, 1: 0.49732686808404825, 2: 2.044989775051125, 3: 1.2351397251814111, 4: 1.3766993632765445, 5: 4.662004662004662}
2025-12-16 16:58:47,889 - INFO - Building model...
2025-12-16 16:58:48,273 - INFO - Model built: GRU, Parameters: 1,604,794
2025-12-16 16:58:48,273 - INFO - Training GRU model...
2025-12-16 17:01:28,712 - INFO - Training completed in 160.44 seconds
2025-12-16 17:01:28,712 - INFO - Evaluating...
2025-12-16 17:01:29,920 - INFO - ================================================================================
2025-12-16 17:01:29,920 - INFO - EXPERIMENT COMPLETED: gru_word2vec_baseline
2025-12-16 17:01:29,921 - INFO -   Validation Accuracy: 0.3485 (34.85%)
2025-12-16 17:01:29,921 - INFO -   Macro F1: 0.0968
2025-12-16 17:01:29,921 - INFO -   Training Time: 160.44s
2025-12-16 17:01:29,921 - INFO - ================================================================================
2025-12-16 17:01:29,925 - INFO - 

Running experiment 5/8: bilstm_glove
2025-12-16 17:01:29,925 - INFO - ================================================================================
2025-12-16 17:01:29,926 - INFO - STARTING EXPERIMENT: bilstm_glove
2025-12-16 17:01:29,926 - INFO - ================================================================================
2025-12-16 17:01:29,938 - INFO - GPUs Available: 0
2025-12-16 17:01:29,939 - INFO - Loading data...
2025-12-16 17:01:29,973 - INFO - Train samples: 16000, Val samples: 2000
2025-12-16 17:01:29,974 - INFO - Preprocessing...
2025-12-16 17:01:33,995 - INFO - Creating GLOVE embeddings...
2025-12-16 17:01:34,501 - INFO - Tokenizer created. Vocabulary size: 15156
2025-12-16 17:01:34,501 - INFO - Loading GloVe embeddings from /home/lab/rabanof/Emotion_Detection_DL/glove/glove.6B.100d.txt...
2025-12-16 17:01:34,501 - ERROR - Experiment bilstm_glove failed: [Errno 2] No such file or directory: '/home/lab/rabanof/Emotion_Detection_DL/glove/glove.6B.100d.txt'
2025-12-16 17:01:34,503 - INFO - 

Running experiment 6/8: lstm_glove_256units
2025-12-16 17:01:34,504 - INFO - ================================================================================
2025-12-16 17:01:34,504 - INFO - STARTING EXPERIMENT: lstm_glove_256units
2025-12-16 17:01:34,504 - INFO - ================================================================================
2025-12-16 17:01:34,504 - INFO - GPUs Available: 0
2025-12-16 17:01:34,504 - INFO - Loading data...
2025-12-16 17:01:34,538 - INFO - Train samples: 16000, Val samples: 2000
2025-12-16 17:01:34,538 - INFO - Preprocessing...
2025-12-16 17:01:38,557 - INFO - Creating GLOVE embeddings...
2025-12-16 17:01:39,074 - INFO - Tokenizer created. Vocabulary size: 15156
2025-12-16 17:01:39,075 - INFO - Loading GloVe embeddings from /home/lab/rabanof/Emotion_Detection_DL/glove/glove.6B.100d.txt...
2025-12-16 17:01:39,075 - ERROR - Experiment lstm_glove_256units failed: [Errno 2] No such file or directory: '/home/lab/rabanof/Emotion_Detection_DL/glove/glove.6B.100d.txt'
2025-12-16 17:01:39,077 - INFO - 

Running experiment 7/8: lstm_glove_2layers
2025-12-16 17:01:39,077 - INFO - ================================================================================
2025-12-16 17:01:39,077 - INFO - STARTING EXPERIMENT: lstm_glove_2layers
2025-12-16 17:01:39,077 - INFO - ================================================================================
2025-12-16 17:01:39,077 - INFO - GPUs Available: 0
2025-12-16 17:01:39,077 - INFO - Loading data...
2025-12-16 17:01:39,111 - INFO - Train samples: 16000, Val samples: 2000
2025-12-16 17:01:39,112 - INFO - Preprocessing...
2025-12-16 17:01:43,130 - INFO - Creating GLOVE embeddings...
2025-12-16 17:01:43,639 - INFO - Tokenizer created. Vocabulary size: 15156
2025-12-16 17:01:43,639 - INFO - Loading GloVe embeddings from /home/lab/rabanof/Emotion_Detection_DL/glove/glove.6B.100d.txt...
2025-12-16 17:01:43,640 - ERROR - Experiment lstm_glove_2layers failed: [Errno 2] No such file or directory: '/home/lab/rabanof/Emotion_Detection_DL/glove/glove.6B.100d.txt'
2025-12-16 17:01:43,641 - INFO - 

Running experiment 8/8: lstm_glove_dropout05
2025-12-16 17:01:43,642 - INFO - ================================================================================
2025-12-16 17:01:43,642 - INFO - STARTING EXPERIMENT: lstm_glove_dropout05
2025-12-16 17:01:43,642 - INFO - ================================================================================
2025-12-16 17:01:43,642 - INFO - GPUs Available: 0
2025-12-16 17:01:43,642 - INFO - Loading data...
2025-12-16 17:01:43,676 - INFO - Train samples: 16000, Val samples: 2000
2025-12-16 17:01:43,676 - INFO - Preprocessing...
2025-12-16 17:01:47,690 - INFO - Creating GLOVE embeddings...
2025-12-16 17:01:48,201 - INFO - Tokenizer created. Vocabulary size: 15156
2025-12-16 17:01:48,201 - INFO - Loading GloVe embeddings from /home/lab/rabanof/Emotion_Detection_DL/glove/glove.6B.100d.txt...
2025-12-16 17:01:48,201 - ERROR - Experiment lstm_glove_dropout05 failed: [Errno 2] No such file or directory: '/home/lab/rabanof/Emotion_Detection_DL/glove/glove.6B.100d.txt'
2025-12-16 17:01:48,203 - INFO - 

================================================================================
2025-12-16 17:01:48,203 - INFO - ALL EXPERIMENTS COMPLETED - COMPARISON TABLE
2025-12-16 17:01:48,203 - INFO - ================================================================================
2025-12-16 17:01:48,348 - INFO - 
Comparison table saved to: results/all_experiments_comparison.csv
2025-12-16 17:01:48,348 - INFO - 
================================================================================
2025-12-16 17:01:48,349 - INFO - BEST MODEL
2025-12-16 17:01:48,349 - INFO - ================================================================================
2025-12-16 17:01:48,349 - INFO - Experiment: gru_word2vec_baseline
2025-12-16 17:01:48,349 - INFO - Model: GRU
2025-12-16 17:01:48,349 - INFO - Embedding: WORD2VEC
2025-12-16 17:01:48,349 - INFO - Accuracy: 0.3485 (34.85%)
2025-12-16 17:01:48,349 - INFO - Macro F1: 0.0968
2025-12-16 17:01:48,349 - INFO - ================================================================================
2025-12-16 17:01:48,349 - INFO - 
All experiments completed successfully!
2025-12-16 17:41:07,461 - INFO - ================================================================================
2025-12-16 17:41:07,461 - INFO - GPU EMOTION DETECTION - EXACT PIPELINE
2025-12-16 17:41:07,461 - INFO - ================================================================================
2025-12-16 17:41:07,461 - INFO - TensorFlow: 2.13.0
2025-12-16 17:41:07,462 - INFO - GPU Devices: []
2025-12-16 17:41:07,462 - INFO - 

EXPERIMENT 1/7: lstm_glove50_baseline
2025-12-16 17:41:07,462 - INFO - ================================================================================
2025-12-16 17:41:07,462 - INFO - EXPERIMENT: lstm_glove50_baseline
2025-12-16 17:41:07,462 - INFO - ================================================================================
2025-12-16 17:41:07,463 - INFO - GPUs Available: 0
2025-12-16 17:41:07,463 - INFO - Step 1: Loading data...
2025-12-16 17:41:07,506 - INFO -   Train: 16000, Val: 2000
2025-12-16 17:41:07,507 - INFO - Step 2: Preprocessing (full_pipeline exact method)...
2025-12-16 17:41:11,966 - INFO -   Removed 34 duplicates
2025-12-16 17:41:12,513 - INFO -   Removed 2 duplicates
2025-12-16 17:41:12,517 - WARNING -   Data leakage: 5 overlapping texts found, removing from train
2025-12-16 17:41:12,521 - INFO - Step 3: Creating GLOVE embeddings...
2025-12-16 17:41:13,018 - INFO -   Tokenizer created. Vocabulary size: 15157
2025-12-16 17:41:13,018 - INFO -   Loading GloVe from /home/lab/rabanof/Emotion_Detection_DL/glove/glove.6B.50d.txt...
2025-12-16 17:41:20,220 - INFO -   Loaded 400000 word vectors
2025-12-16 17:41:20,252 - INFO -   Embedding coverage: 93.68% (14199/15157)
2025-12-16 17:41:20,252 - INFO - Step 4: Converting to sequences...
2025-12-16 17:41:20,718 - INFO -   X_train shape: (15961, 60), y_train shape: (15961, 6)
2025-12-16 17:41:20,723 - INFO -   Class weights: {0: 0.5706063206063207, 1: 0.497599451303155, 2: 2.0510151631971216, 3: 1.234416086620263, 4: 1.3761855492326263, 5: 4.683392018779343}
2025-12-16 17:41:20,723 - INFO - Step 5: Building model...
2025-12-16 17:41:21,015 - INFO -   Model built: LSTM, Parameters: 850,322
2025-12-16 17:41:21,016 - INFO - Step 6: Training LSTM model...
2025-12-16 18:02:36,310 - INFO -   Training completed in 1275.29 seconds
2025-12-16 18:02:36,310 - INFO - Step 7: Evaluating...
2025-12-16 18:02:37,276 - INFO - ================================================================================
2025-12-16 18:02:37,276 - INFO - COMPLETED: lstm_glove50_baseline
2025-12-16 18:02:37,276 - INFO -   Val Accuracy: 0.7102 (71.02%)
2025-12-16 18:02:37,276 - INFO -   Macro F1: 0.7021
2025-12-16 18:02:37,276 - INFO -   Training Time: 1275.29s
2025-12-16 18:02:37,276 - INFO - ================================================================================
2025-12-16 18:02:37,357 - INFO - 

EXPERIMENT 2/7: gru_glove50_baseline
2025-12-16 18:02:37,357 - INFO - ================================================================================
2025-12-16 18:02:37,357 - INFO - EXPERIMENT: gru_glove50_baseline
2025-12-16 18:02:37,357 - INFO - ================================================================================
2025-12-16 18:02:37,373 - INFO - GPUs Available: 0
2025-12-16 18:02:37,374 - INFO - Step 1: Loading data...
2025-12-16 18:02:37,408 - INFO -   Train: 16000, Val: 2000
2025-12-16 18:02:37,408 - INFO - Step 2: Preprocessing (full_pipeline exact method)...
2025-12-16 18:02:41,779 - INFO -   Removed 34 duplicates
2025-12-16 18:02:42,322 - INFO -   Removed 2 duplicates
2025-12-16 18:02:42,326 - WARNING -   Data leakage: 5 overlapping texts found, removing from train
2025-12-16 18:02:42,330 - INFO - Step 3: Creating GLOVE embeddings...
2025-12-16 18:02:42,827 - INFO -   Tokenizer created. Vocabulary size: 15157
2025-12-16 18:02:42,827 - INFO -   Loading GloVe from /home/lab/rabanof/Emotion_Detection_DL/glove/glove.6B.50d.txt...
2025-12-16 18:02:49,898 - INFO -   Loaded 400000 word vectors
2025-12-16 18:02:49,926 - INFO -   Embedding coverage: 93.68% (14199/15157)
2025-12-16 18:02:49,926 - INFO - Step 4: Converting to sequences...
2025-12-16 18:02:50,394 - INFO -   X_train shape: (15961, 60), y_train shape: (15961, 6)
2025-12-16 18:02:50,398 - INFO -   Class weights: {0: 0.5706063206063207, 1: 0.497599451303155, 2: 2.0510151631971216, 3: 1.234416086620263, 4: 1.3761855492326263, 5: 4.683392018779343}
2025-12-16 18:02:50,398 - INFO - Step 5: Building model...
2025-12-16 18:02:50,657 - INFO -   Model built: GRU, Parameters: 827,794
2025-12-16 18:02:50,657 - INFO - Step 6: Training GRU model...
2025-12-16 18:07:57,213 - INFO -   Training completed in 306.56 seconds
2025-12-16 18:07:57,213 - INFO - Step 7: Evaluating...
2025-12-16 18:07:58,159 - INFO - ================================================================================
2025-12-16 18:07:58,159 - INFO - COMPLETED: gru_glove50_baseline
2025-12-16 18:07:58,159 - INFO -   Val Accuracy: 0.3493 (34.93%)
2025-12-16 18:07:58,159 - INFO -   Macro F1: 0.0953
2025-12-16 18:07:58,159 - INFO -   Training Time: 306.56s
2025-12-16 18:07:58,159 - INFO - ================================================================================
2025-12-16 18:07:58,238 - INFO - 

EXPERIMENT 3/7: lstm_word2vec_baseline
2025-12-16 18:07:58,239 - INFO - ================================================================================
2025-12-16 18:07:58,239 - INFO - EXPERIMENT: lstm_word2vec_baseline
2025-12-16 18:07:58,239 - INFO - ================================================================================
2025-12-16 18:07:58,256 - INFO - GPUs Available: 0
2025-12-16 18:07:58,257 - INFO - Step 1: Loading data...
2025-12-16 18:07:58,294 - INFO -   Train: 16000, Val: 2000
2025-12-16 18:07:58,294 - INFO - Step 2: Preprocessing (full_pipeline exact method)...
2025-12-16 18:08:02,642 - INFO -   Removed 34 duplicates
2025-12-16 18:08:03,184 - INFO -   Removed 2 duplicates
2025-12-16 18:08:03,188 - WARNING -   Data leakage: 5 overlapping texts found, removing from train
2025-12-16 18:08:03,192 - INFO - Step 3: Creating WORD2VEC embeddings...
2025-12-16 18:08:03,694 - INFO -   Tokenizer created. Vocabulary size: 15157
2025-12-16 18:08:03,694 - INFO -   Training Word2Vec embeddings...
2025-12-16 18:08:03,734 - INFO - collecting all words and their counts
2025-12-16 18:08:03,734 - INFO - PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
2025-12-16 18:08:03,792 - INFO - PROGRESS: at sentence #10000, processed 195943 words, keeping 11936 word types
2025-12-16 18:08:03,826 - INFO - collected 15156 word types from a corpus of 310407 raw words and 15961 sentences
2025-12-16 18:08:03,826 - INFO - Creating a fresh vocabulary
2025-12-16 18:08:03,903 - INFO - Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 15156 unique words (100.00% of original 15156, drops 0)', 'datetime': '2025-12-16T18:08:03.903412', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'prepare_vocab'}
2025-12-16 18:08:03,903 - INFO - Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 310407 word corpus (100.00% of original 310407, drops 0)', 'datetime': '2025-12-16T18:08:03.903692', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'prepare_vocab'}
2025-12-16 18:08:04,025 - INFO - deleting the raw counts dictionary of 15156 items
2025-12-16 18:08:04,026 - INFO - sample=0.001 downsamples 48 most-common words
2025-12-16 18:08:04,026 - INFO - Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 209815.12000829354 word corpus (67.6%% of prior 310407)', 'datetime': '2025-12-16T18:08:04.026228', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'prepare_vocab'}
2025-12-16 18:08:04,226 - INFO - estimated required memory for 15156 words and 50 dimensions: 13640400 bytes
2025-12-16 18:08:04,227 - INFO - resetting layer weights
2025-12-16 18:08:04,236 - INFO - Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-12-16T18:08:04.236705', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'build_vocab'}
2025-12-16 18:08:04,236 - INFO - Word2Vec lifecycle event {'msg': 'training model with 4 workers on 15156 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-12-16T18:08:04.236973', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'train'}
2025-12-16 18:08:04,424 - INFO - EPOCH 0: training on 310407 raw words (209849 effective words) took 0.2s, 1162139 effective words/s
2025-12-16 18:08:04,607 - INFO - EPOCH 1: training on 310407 raw words (209413 effective words) took 0.2s, 1185030 effective words/s
2025-12-16 18:08:04,795 - INFO - EPOCH 2: training on 310407 raw words (209788 effective words) took 0.2s, 1150924 effective words/s
2025-12-16 18:08:04,978 - INFO - EPOCH 3: training on 310407 raw words (209718 effective words) took 0.2s, 1189927 effective words/s
2025-12-16 18:08:05,162 - INFO - EPOCH 4: training on 310407 raw words (209968 effective words) took 0.2s, 1180516 effective words/s
2025-12-16 18:08:05,163 - INFO - Word2Vec lifecycle event {'msg': 'training on 1552035 raw words (1048736 effective words) took 0.9s, 1132611 effective words/s', 'datetime': '2025-12-16T18:08:05.163076', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'train'}
2025-12-16 18:08:05,163 - INFO - Word2Vec lifecycle event {'params': 'Word2Vec<vocab=15156, vector_size=50, alpha=0.025>', 'datetime': '2025-12-16T18:08:05.163264', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'created'}
2025-12-16 18:08:05,163 - INFO -   Word2Vec trained. Vocabulary: 15156
2025-12-16 18:08:05,226 - INFO -   Embedding coverage: 99.99% (15156/15157)
2025-12-16 18:08:05,227 - INFO - Step 4: Converting to sequences...
2025-12-16 18:08:05,692 - INFO -   X_train shape: (15961, 60), y_train shape: (15961, 6)
2025-12-16 18:08:05,697 - INFO -   Class weights: {0: 0.5706063206063207, 1: 0.497599451303155, 2: 2.0510151631971216, 3: 1.234416086620263, 4: 1.3761855492326263, 5: 4.683392018779343}
2025-12-16 18:08:05,697 - INFO - Step 5: Building model...
2025-12-16 18:08:05,936 - INFO -   Model built: LSTM, Parameters: 850,322
2025-12-16 18:08:05,936 - INFO - Step 6: Training LSTM model...
2025-12-16 18:12:48,906 - INFO -   Training completed in 282.97 seconds
2025-12-16 18:12:48,906 - INFO - Step 7: Evaluating...
2025-12-16 18:12:49,888 - INFO - ================================================================================
2025-12-16 18:12:49,888 - INFO - COMPLETED: lstm_word2vec_baseline
2025-12-16 18:12:49,888 - INFO -   Val Accuracy: 0.2362 (23.62%)
2025-12-16 18:12:49,888 - INFO -   Macro F1: 0.0978
2025-12-16 18:12:49,888 - INFO -   Training Time: 282.97s
2025-12-16 18:12:49,888 - INFO - ================================================================================
2025-12-16 18:12:49,892 - INFO - 

EXPERIMENT 4/7: gru_word2vec_baseline
2025-12-16 18:12:49,892 - INFO - ================================================================================
2025-12-16 18:12:49,892 - INFO - EXPERIMENT: gru_word2vec_baseline
2025-12-16 18:12:49,892 - INFO - ================================================================================
2025-12-16 18:12:49,909 - INFO - GPUs Available: 0
2025-12-16 18:12:49,910 - INFO - Step 1: Loading data...
2025-12-16 18:12:49,947 - INFO -   Train: 16000, Val: 2000
2025-12-16 18:12:49,947 - INFO - Step 2: Preprocessing (full_pipeline exact method)...
2025-12-16 18:12:54,292 - INFO -   Removed 34 duplicates
2025-12-16 18:12:54,834 - INFO -   Removed 2 duplicates
2025-12-16 18:12:54,839 - WARNING -   Data leakage: 5 overlapping texts found, removing from train
2025-12-16 18:12:54,843 - INFO - Step 3: Creating WORD2VEC embeddings...
2025-12-16 18:12:55,348 - INFO -   Tokenizer created. Vocabulary size: 15157
2025-12-16 18:12:55,348 - INFO -   Training Word2Vec embeddings...
2025-12-16 18:12:55,383 - INFO - collecting all words and their counts
2025-12-16 18:12:55,384 - INFO - PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
2025-12-16 18:12:55,441 - INFO - PROGRESS: at sentence #10000, processed 195943 words, keeping 11936 word types
2025-12-16 18:12:55,475 - INFO - collected 15156 word types from a corpus of 310407 raw words and 15961 sentences
2025-12-16 18:12:55,475 - INFO - Creating a fresh vocabulary
2025-12-16 18:12:55,550 - INFO - Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 15156 unique words (100.00% of original 15156, drops 0)', 'datetime': '2025-12-16T18:12:55.550862', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'prepare_vocab'}
2025-12-16 18:12:55,551 - INFO - Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 310407 word corpus (100.00% of original 310407, drops 0)', 'datetime': '2025-12-16T18:12:55.551159', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'prepare_vocab'}
2025-12-16 18:12:55,672 - INFO - deleting the raw counts dictionary of 15156 items
2025-12-16 18:12:55,673 - INFO - sample=0.001 downsamples 48 most-common words
2025-12-16 18:12:55,673 - INFO - Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 209815.12000829354 word corpus (67.6%% of prior 310407)', 'datetime': '2025-12-16T18:12:55.673292', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'prepare_vocab'}
2025-12-16 18:12:55,874 - INFO - estimated required memory for 15156 words and 50 dimensions: 13640400 bytes
2025-12-16 18:12:55,875 - INFO - resetting layer weights
2025-12-16 18:12:55,880 - INFO - Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-12-16T18:12:55.880819', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'build_vocab'}
2025-12-16 18:12:55,881 - INFO - Word2Vec lifecycle event {'msg': 'training model with 4 workers on 15156 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-12-16T18:12:55.881048', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'train'}
2025-12-16 18:12:56,067 - INFO - EPOCH 0: training on 310407 raw words (209849 effective words) took 0.2s, 1167129 effective words/s
2025-12-16 18:12:56,253 - INFO - EPOCH 1: training on 310407 raw words (209413 effective words) took 0.2s, 1163495 effective words/s
2025-12-16 18:12:56,438 - INFO - EPOCH 2: training on 310407 raw words (209990 effective words) took 0.2s, 1196127 effective words/s
2025-12-16 18:12:56,625 - INFO - EPOCH 3: training on 310407 raw words (209824 effective words) took 0.2s, 1162997 effective words/s
2025-12-16 18:12:56,810 - INFO - EPOCH 4: training on 310407 raw words (209764 effective words) took 0.2s, 1171595 effective words/s
2025-12-16 18:12:56,811 - INFO - Word2Vec lifecycle event {'msg': 'training on 1552035 raw words (1048840 effective words) took 0.9s, 1127781 effective words/s', 'datetime': '2025-12-16T18:12:56.811209', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'train'}
2025-12-16 18:12:56,811 - INFO - Word2Vec lifecycle event {'params': 'Word2Vec<vocab=15156, vector_size=50, alpha=0.025>', 'datetime': '2025-12-16T18:12:56.811401', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'created'}
2025-12-16 18:12:56,811 - INFO -   Word2Vec trained. Vocabulary: 15156
2025-12-16 18:12:56,874 - INFO -   Embedding coverage: 99.99% (15156/15157)
2025-12-16 18:12:56,874 - INFO - Step 4: Converting to sequences...
2025-12-16 18:12:57,342 - INFO -   X_train shape: (15961, 60), y_train shape: (15961, 6)
2025-12-16 18:12:57,347 - INFO -   Class weights: {0: 0.5706063206063207, 1: 0.497599451303155, 2: 2.0510151631971216, 3: 1.234416086620263, 4: 1.3761855492326263, 5: 4.683392018779343}
2025-12-16 18:12:57,347 - INFO - Step 5: Building model...
2025-12-16 18:12:57,606 - INFO -   Model built: GRU, Parameters: 827,794
2025-12-16 18:12:57,607 - INFO - Step 6: Training GRU model...
2025-12-16 18:16:18,140 - INFO -   Training completed in 200.53 seconds
2025-12-16 18:16:18,140 - INFO - Step 7: Evaluating...
2025-12-16 18:16:19,088 - INFO - ================================================================================
2025-12-16 18:16:19,088 - INFO - COMPLETED: gru_word2vec_baseline
2025-12-16 18:16:19,088 - INFO -   Val Accuracy: 0.1071 (10.71%)
2025-12-16 18:16:19,088 - INFO -   Macro F1: 0.0370
2025-12-16 18:16:19,088 - INFO -   Training Time: 200.53s
2025-12-16 18:16:19,088 - INFO - ================================================================================
2025-12-16 18:16:19,093 - INFO - 

EXPERIMENT 5/7: bilstm_glove50
2025-12-16 18:16:19,093 - INFO - ================================================================================
2025-12-16 18:16:19,093 - INFO - EXPERIMENT: bilstm_glove50
2025-12-16 18:16:19,093 - INFO - ================================================================================
2025-12-16 18:16:19,109 - INFO - GPUs Available: 0
2025-12-16 18:16:19,110 - INFO - Step 1: Loading data...
2025-12-16 18:16:19,151 - INFO -   Train: 16000, Val: 2000
2025-12-16 18:16:19,151 - INFO - Step 2: Preprocessing (full_pipeline exact method)...
2025-12-16 18:16:23,500 - INFO -   Removed 34 duplicates
2025-12-16 18:16:24,043 - INFO -   Removed 2 duplicates
2025-12-16 18:16:24,048 - WARNING -   Data leakage: 5 overlapping texts found, removing from train
2025-12-16 18:16:24,052 - INFO - Step 3: Creating GLOVE embeddings...
2025-12-16 18:16:24,563 - INFO -   Tokenizer created. Vocabulary size: 15157
2025-12-16 18:16:24,563 - INFO -   Loading GloVe from /home/lab/rabanof/Emotion_Detection_DL/glove/glove.6B.50d.txt...
2025-12-16 18:16:31,726 - INFO -   Loaded 400000 word vectors
2025-12-16 18:16:31,756 - INFO -   Embedding coverage: 93.68% (14199/15157)
2025-12-16 18:16:31,756 - INFO - Step 4: Converting to sequences...
2025-12-16 18:16:32,232 - INFO -   X_train shape: (15961, 60), y_train shape: (15961, 6)
2025-12-16 18:16:32,237 - INFO -   Class weights: {0: 0.5706063206063207, 1: 0.497599451303155, 2: 2.0510151631971216, 3: 1.234416086620263, 4: 1.3761855492326263, 5: 4.683392018779343}
2025-12-16 18:16:32,238 - INFO - Step 5: Building model...
2025-12-16 18:16:32,935 - INFO -   Model built: BILSTM, Parameters: 942,738
2025-12-16 18:16:32,935 - INFO - Step 6: Training BILSTM model...
2025-12-16 18:31:07,152 - INFO -   Training completed in 874.22 seconds
2025-12-16 18:31:07,152 - INFO - Step 7: Evaluating...
2025-12-16 18:31:09,265 - INFO - ================================================================================
2025-12-16 18:31:09,265 - INFO - COMPLETED: bilstm_glove50
2025-12-16 18:31:09,265 - INFO -   Val Accuracy: 0.8624 (86.24%)
2025-12-16 18:31:09,266 - INFO -   Macro F1: 0.8395
2025-12-16 18:31:09,266 - INFO -   Training Time: 874.22s
2025-12-16 18:31:09,266 - INFO - ================================================================================
2025-12-16 18:31:09,343 - INFO - 

EXPERIMENT 6/7: lstm_glove50_256units
2025-12-16 18:31:09,345 - INFO - ================================================================================
2025-12-16 18:31:09,345 - INFO - EXPERIMENT: lstm_glove50_256units
2025-12-16 18:31:09,345 - INFO - ================================================================================
2025-12-16 18:31:09,371 - INFO - GPUs Available: 0
2025-12-16 18:31:09,372 - INFO - Step 1: Loading data...
2025-12-16 18:31:09,408 - INFO -   Train: 16000, Val: 2000
2025-12-16 18:31:09,409 - INFO - Step 2: Preprocessing (full_pipeline exact method)...
2025-12-16 18:31:13,756 - INFO -   Removed 34 duplicates
2025-12-16 18:31:14,297 - INFO -   Removed 2 duplicates
2025-12-16 18:31:14,302 - WARNING -   Data leakage: 5 overlapping texts found, removing from train
2025-12-16 18:31:14,306 - INFO - Step 3: Creating GLOVE embeddings...
2025-12-16 18:31:14,813 - INFO -   Tokenizer created. Vocabulary size: 15157
2025-12-16 18:31:14,814 - INFO -   Loading GloVe from /home/lab/rabanof/Emotion_Detection_DL/glove/glove.6B.50d.txt...
2025-12-16 18:31:21,899 - INFO -   Loaded 400000 word vectors
2025-12-16 18:31:21,929 - INFO -   Embedding coverage: 93.68% (14199/15157)
2025-12-16 18:31:21,929 - INFO - Step 4: Converting to sequences...
2025-12-16 18:31:22,395 - INFO -   X_train shape: (15961, 60), y_train shape: (15961, 6)
2025-12-16 18:31:22,400 - INFO -   Class weights: {0: 0.5706063206063207, 1: 0.497599451303155, 2: 2.0510151631971216, 3: 1.234416086620263, 4: 1.3761855492326263, 5: 4.683392018779343}
2025-12-16 18:31:22,400 - INFO - Step 5: Building model...
2025-12-16 18:31:22,691 - INFO -   Model built: LSTM, Parameters: 1,073,810
2025-12-16 18:31:22,691 - INFO - Step 6: Training LSTM model...
2025-12-16 18:39:33,438 - INFO -   Training completed in 490.75 seconds
2025-12-16 18:39:33,438 - INFO - Step 7: Evaluating...
2025-12-16 18:39:35,135 - INFO - ================================================================================
2025-12-16 18:39:35,135 - INFO - COMPLETED: lstm_glove50_256units
2025-12-16 18:39:35,136 - INFO -   Val Accuracy: 0.1286 (12.86%)
2025-12-16 18:39:35,136 - INFO -   Macro F1: 0.0664
2025-12-16 18:39:35,136 - INFO -   Training Time: 490.75s
2025-12-16 18:39:35,136 - INFO - ================================================================================
2025-12-16 18:39:35,218 - INFO - 

EXPERIMENT 7/7: lstm_glove50_dropout05
2025-12-16 18:39:35,220 - INFO - ================================================================================
2025-12-16 18:39:35,220 - INFO - EXPERIMENT: lstm_glove50_dropout05
2025-12-16 18:39:35,220 - INFO - ================================================================================
2025-12-16 18:39:35,237 - INFO - GPUs Available: 0
2025-12-16 18:39:35,238 - INFO - Step 1: Loading data...
2025-12-16 18:39:35,275 - INFO -   Train: 16000, Val: 2000
2025-12-16 18:39:35,275 - INFO - Step 2: Preprocessing (full_pipeline exact method)...
2025-12-16 18:39:39,639 - INFO -   Removed 34 duplicates
2025-12-16 18:39:40,178 - INFO -   Removed 2 duplicates
2025-12-16 18:39:40,182 - WARNING -   Data leakage: 5 overlapping texts found, removing from train
2025-12-16 18:39:40,186 - INFO - Step 3: Creating GLOVE embeddings...
2025-12-16 18:39:40,703 - INFO -   Tokenizer created. Vocabulary size: 15157
2025-12-16 18:39:40,703 - INFO -   Loading GloVe from /home/lab/rabanof/Emotion_Detection_DL/glove/glove.6B.50d.txt...
2025-12-16 18:39:47,791 - INFO -   Loaded 400000 word vectors
2025-12-16 18:39:47,821 - INFO -   Embedding coverage: 93.68% (14199/15157)
2025-12-16 18:39:47,821 - INFO - Step 4: Converting to sequences...
2025-12-16 18:39:48,291 - INFO -   X_train shape: (15961, 60), y_train shape: (15961, 6)
2025-12-16 18:39:48,295 - INFO -   Class weights: {0: 0.5706063206063207, 1: 0.497599451303155, 2: 2.0510151631971216, 3: 1.234416086620263, 4: 1.3761855492326263, 5: 4.683392018779343}
2025-12-16 18:39:48,296 - INFO - Step 5: Building model...
2025-12-16 18:39:48,536 - INFO -   Model built: LSTM, Parameters: 850,322
2025-12-16 18:39:48,537 - INFO - Step 6: Training LSTM model...
2025-12-16 18:45:21,966 - INFO -   Training completed in 333.43 seconds
2025-12-16 18:45:21,966 - INFO - Step 7: Evaluating...
2025-12-16 18:46:15,230 - INFO - ================================================================================
2025-12-16 18:46:15,230 - INFO - COMPLETED: lstm_glove50_dropout05
2025-12-16 18:46:15,230 - INFO -   Val Accuracy: 0.3068 (30.68%)
2025-12-16 18:46:15,230 - INFO -   Macro F1: 0.1424
2025-12-16 18:46:15,230 - INFO -   Training Time: 333.43s
2025-12-16 18:46:15,230 - INFO - ================================================================================
2025-12-16 18:46:15,312 - INFO - 

================================================================================
2025-12-16 18:46:15,315 - INFO - ALL EXPERIMENTS COMPLETED - COMPARISON TABLE
2025-12-16 18:46:15,315 - INFO - ================================================================================
2025-12-16 18:46:15,391 - INFO - 
Comparison saved to: results/all_experiments_comparison.csv
2025-12-16 18:46:15,391 - INFO - 
================================================================================
2025-12-16 18:46:15,392 - INFO - BEST MODEL
2025-12-16 18:46:15,392 - INFO - ================================================================================
2025-12-16 18:46:15,392 - INFO - Experiment: bilstm_glove50
2025-12-16 18:46:15,392 - INFO - Model: BILSTM
2025-12-16 18:46:15,392 - INFO - Embedding: GLOVE 50d
2025-12-16 18:46:15,392 - INFO - Accuracy: 0.8624 (86.24%)
2025-12-16 18:46:15,392 - INFO - Macro F1: 0.8395
2025-12-16 18:46:15,392 - INFO - ================================================================================
2025-12-16 18:46:15,392 - INFO - 
All experiments completed successfully!
2025-12-17 12:52:58,759 - INFO - ================================================================================
2025-12-17 12:52:58,759 - INFO - GPU EMOTION DETECTION - EXACT PIPELINE
2025-12-17 12:52:58,759 - INFO - ================================================================================
2025-12-17 12:52:58,760 - INFO - TensorFlow: 2.13.0
2025-12-17 12:52:58,760 - INFO - GPU Devices: []
2025-12-17 12:52:58,760 - INFO - 

EXPERIMENT 1/7: lstm_glove50_baseline
2025-12-17 12:52:58,760 - INFO - ================================================================================
2025-12-17 12:52:58,760 - INFO - EXPERIMENT: lstm_glove50_baseline
2025-12-17 12:52:58,760 - INFO - ================================================================================
2025-12-17 12:52:58,761 - INFO - GPUs Available: 0
2025-12-17 12:52:58,761 - INFO - Step 1: Loading data...
2025-12-17 12:52:58,805 - INFO -   Train: 16000, Val: 2000
2025-12-17 12:52:58,805 - INFO - Step 2: Preprocessing (full_pipeline exact method)...
2025-12-17 12:53:03,240 - INFO -   Removed 34 duplicates
2025-12-17 12:53:03,787 - INFO -   Removed 2 duplicates
2025-12-17 12:53:03,791 - WARNING -   Data leakage: 5 overlapping texts found, removing from train
2025-12-17 12:53:03,795 - INFO - Step 3: Creating GLOVE embeddings...
2025-12-17 12:53:04,298 - INFO -   Tokenizer created. Vocabulary size: 15157
2025-12-17 12:53:04,298 - INFO -   Loading GloVe from /home/lab/rabanof/Emotion_Detection_DL/glove/glove.6B.50d.txt...
2025-12-17 12:53:11,675 - INFO -   Loaded 400000 word vectors
2025-12-17 12:53:11,708 - INFO -   Embedding coverage: 93.68% (14199/15157)
2025-12-17 12:53:11,708 - INFO - Step 4: Converting to sequences...
2025-12-17 12:53:12,178 - INFO -   X_train shape: (15961, 60), y_train shape: (15961, 6)
2025-12-17 12:53:12,218 - INFO -   Class weights: {0: 0.5706063206063207, 1: 0.497599451303155, 2: 2.0510151631971216, 3: 1.234416086620263, 4: 1.3761855492326263, 5: 4.683392018779343}
2025-12-17 12:53:12,218 - INFO - Step 5: Building model...
2025-12-17 12:53:13,047 - INFO -   Model built: LSTM, Parameters: 850,322
2025-12-17 12:53:13,047 - INFO - Step 6: Training LSTM model...
2025-12-17 15:07:53,482 - INFO - ================================================================================
2025-12-17 15:07:53,482 - INFO - GPU EMOTION DETECTION - EXACT PIPELINE
2025-12-17 15:07:53,482 - INFO - ================================================================================
2025-12-17 15:07:53,482 - INFO - TensorFlow: 2.13.0
2025-12-17 15:07:53,879 - INFO - GPU Devices: []
2025-12-17 15:07:53,879 - INFO - 

EXPERIMENT 1/7: lstm_glove50_baseline
2025-12-17 15:07:53,879 - INFO - ================================================================================
2025-12-17 15:07:53,879 - INFO - EXPERIMENT: lstm_glove50_baseline
2025-12-17 15:07:53,879 - INFO - ================================================================================
2025-12-17 15:07:53,879 - INFO - GPUs Available: 0
2025-12-17 15:07:53,880 - INFO - Step 1: Loading data...
2025-12-17 15:07:53,918 - INFO -   Train: 16000, Val: 2000
2025-12-17 15:07:53,918 - INFO - Step 2: Preprocessing (full_pipeline exact method)...
2025-12-17 15:07:56,372 - INFO -   Removed 34 duplicates
2025-12-17 15:07:56,677 - INFO -   Removed 2 duplicates
2025-12-17 15:07:56,680 - WARNING -   Data leakage: 5 overlapping texts found, removing from train
2025-12-17 15:07:56,682 - INFO - Step 3: Creating GLOVE embeddings...
2025-12-17 15:07:56,956 - INFO -   Tokenizer created. Vocabulary size: 15157
2025-12-17 15:07:56,956 - INFO -   Loading GloVe from /home/lab/rabanof/Emotion_Detection_DL/glove/glove.6B.50d.txt...
2025-12-17 15:08:00,802 - INFO -   Loaded 400000 word vectors
2025-12-17 15:08:00,834 - INFO -   Embedding coverage: 93.68% (14199/15157)
2025-12-17 15:08:00,834 - INFO - Step 4: Converting to sequences...
2025-12-17 15:08:01,093 - INFO -   X_train shape: (15961, 60), y_train shape: (15961, 6)
2025-12-17 15:08:01,098 - INFO -   Class weights: {0: 0.5706063206063207, 1: 0.497599451303155, 2: 2.0510151631971216, 3: 1.234416086620263, 4: 1.3761855492326263, 5: 4.683392018779343}
2025-12-17 15:08:01,098 - INFO - Step 5: Building model...
2025-12-17 15:08:01,329 - INFO -   Model built: LSTM, Parameters: 850,322
2025-12-17 15:08:01,329 - INFO - Step 6: Training LSTM model...
2025-12-17 15:25:32,949 - INFO -   Training completed in 1051.62 seconds
2025-12-17 15:25:32,949 - INFO - Step 7: Evaluating...
2025-12-17 15:25:33,875 - INFO - ================================================================================
2025-12-17 15:25:33,875 - INFO - COMPLETED: lstm_glove50_baseline
2025-12-17 15:25:33,875 - INFO -   Val Accuracy: 0.8333 (83.33%)
2025-12-17 15:25:33,875 - INFO -   Macro F1: 0.8201
2025-12-17 15:25:33,875 - INFO -   Training Time: 1051.62s
2025-12-17 15:25:33,875 - INFO - ================================================================================
2025-12-17 15:25:33,989 - INFO - 

EXPERIMENT 2/7: gru_glove50_baseline
2025-12-17 15:25:33,989 - INFO - ================================================================================
2025-12-17 15:25:33,990 - INFO - EXPERIMENT: gru_glove50_baseline
2025-12-17 15:25:33,990 - INFO - ================================================================================
2025-12-17 15:25:34,006 - INFO - GPUs Available: 0
2025-12-17 15:25:34,007 - INFO - Step 1: Loading data...
2025-12-17 15:25:34,034 - INFO -   Train: 16000, Val: 2000
2025-12-17 15:25:34,034 - INFO - Step 2: Preprocessing (full_pipeline exact method)...
2025-12-17 15:25:36,429 - INFO -   Removed 34 duplicates
2025-12-17 15:25:36,732 - INFO -   Removed 2 duplicates
2025-12-17 15:25:36,735 - WARNING -   Data leakage: 5 overlapping texts found, removing from train
2025-12-17 15:25:36,737 - INFO - Step 3: Creating GLOVE embeddings...
2025-12-17 15:25:37,017 - INFO -   Tokenizer created. Vocabulary size: 15157
2025-12-17 15:25:37,017 - INFO -   Loading GloVe from /home/lab/rabanof/Emotion_Detection_DL/glove/glove.6B.50d.txt...
2025-12-17 15:25:40,885 - INFO -   Loaded 400000 word vectors
2025-12-17 15:25:40,925 - INFO -   Embedding coverage: 93.68% (14199/15157)
2025-12-17 15:25:40,925 - INFO - Step 4: Converting to sequences...
2025-12-17 15:25:41,184 - INFO -   X_train shape: (15961, 60), y_train shape: (15961, 6)
2025-12-17 15:25:41,188 - INFO -   Class weights: {0: 0.5706063206063207, 1: 0.497599451303155, 2: 2.0510151631971216, 3: 1.234416086620263, 4: 1.3761855492326263, 5: 4.683392018779343}
2025-12-17 15:25:41,188 - INFO - Step 5: Building model...
2025-12-17 15:25:41,350 - INFO -   Model built: GRU, Parameters: 827,794
2025-12-17 15:25:41,351 - INFO - Step 6: Training GRU model...
2025-12-17 15:31:45,811 - INFO -   Training completed in 364.46 seconds
2025-12-17 15:31:45,812 - INFO - Step 7: Evaluating...
2025-12-17 15:31:46,829 - INFO - ================================================================================
2025-12-17 15:31:46,830 - INFO - COMPLETED: gru_glove50_baseline
2025-12-17 15:31:46,830 - INFO -   Val Accuracy: 0.1066 (10.66%)
2025-12-17 15:31:46,830 - INFO -   Macro F1: 0.0330
2025-12-17 15:31:46,830 - INFO -   Training Time: 364.46s
2025-12-17 15:31:46,830 - INFO - ================================================================================
2025-12-17 15:31:46,947 - INFO - 

EXPERIMENT 3/7: lstm_word2vec_baseline
2025-12-17 15:31:46,948 - INFO - ================================================================================
2025-12-17 15:31:46,948 - INFO - EXPERIMENT: lstm_word2vec_baseline
2025-12-17 15:31:46,948 - INFO - ================================================================================
2025-12-17 15:31:46,965 - INFO - GPUs Available: 0
2025-12-17 15:31:46,966 - INFO - Step 1: Loading data...
2025-12-17 15:31:46,994 - INFO -   Train: 16000, Val: 2000
2025-12-17 15:31:46,994 - INFO - Step 2: Preprocessing (full_pipeline exact method)...
2025-12-17 15:31:49,367 - INFO -   Removed 34 duplicates
2025-12-17 15:31:49,668 - INFO -   Removed 2 duplicates
2025-12-17 15:31:49,671 - WARNING -   Data leakage: 5 overlapping texts found, removing from train
2025-12-17 15:31:49,673 - INFO - Step 3: Creating WORD2VEC embeddings...
2025-12-17 15:31:49,946 - INFO -   Tokenizer created. Vocabulary size: 15157
2025-12-17 15:31:49,946 - INFO -   Training Word2Vec embeddings...
2025-12-17 15:31:49,976 - INFO - collecting all words and their counts
2025-12-17 15:31:49,976 - INFO - PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
2025-12-17 15:31:50,007 - INFO - PROGRESS: at sentence #10000, processed 195943 words, keeping 11936 word types
2025-12-17 15:31:50,025 - INFO - collected 15156 word types from a corpus of 310407 raw words and 15961 sentences
2025-12-17 15:31:50,025 - INFO - Creating a fresh vocabulary
2025-12-17 15:31:50,070 - INFO - Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 15156 unique words (100.00% of original 15156, drops 0)', 'datetime': '2025-12-17T15:31:50.070384', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'prepare_vocab'}
2025-12-17 15:31:50,070 - INFO - Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 310407 word corpus (100.00% of original 310407, drops 0)', 'datetime': '2025-12-17T15:31:50.070761', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'prepare_vocab'}
2025-12-17 15:31:50,136 - INFO - deleting the raw counts dictionary of 15156 items
2025-12-17 15:31:50,136 - INFO - sample=0.001 downsamples 48 most-common words
2025-12-17 15:31:50,136 - INFO - Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 209815.12000829354 word corpus (67.6%% of prior 310407)', 'datetime': '2025-12-17T15:31:50.136468', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'prepare_vocab'}
2025-12-17 15:31:50,244 - INFO - estimated required memory for 15156 words and 50 dimensions: 13640400 bytes
2025-12-17 15:31:50,244 - INFO - resetting layer weights
2025-12-17 15:31:50,247 - INFO - Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-12-17T15:31:50.247296', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'build_vocab'}
2025-12-17 15:31:50,247 - INFO - Word2Vec lifecycle event {'msg': 'training model with 4 workers on 15156 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-12-17T15:31:50.247471', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'train'}
2025-12-17 15:31:50,387 - INFO - EPOCH 0: training on 310407 raw words (209752 effective words) took 0.1s, 1554586 effective words/s
2025-12-17 15:31:50,552 - INFO - EPOCH 1: training on 310407 raw words (209604 effective words) took 0.2s, 1305090 effective words/s
2025-12-17 15:31:50,722 - INFO - EPOCH 2: training on 310407 raw words (209612 effective words) took 0.2s, 1272863 effective words/s
2025-12-17 15:31:50,883 - INFO - EPOCH 3: training on 310407 raw words (209797 effective words) took 0.2s, 1340473 effective words/s
2025-12-17 15:31:50,988 - INFO - EPOCH 4: training on 310407 raw words (209764 effective words) took 0.1s, 2091782 effective words/s
2025-12-17 15:31:50,988 - INFO - Word2Vec lifecycle event {'msg': 'training on 1552035 raw words (1048529 effective words) took 0.7s, 1415491 effective words/s', 'datetime': '2025-12-17T15:31:50.988331', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'train'}
2025-12-17 15:31:50,988 - INFO - Word2Vec lifecycle event {'params': 'Word2Vec<vocab=15156, vector_size=50, alpha=0.025>', 'datetime': '2025-12-17T15:31:50.988508', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'created'}
2025-12-17 15:31:50,988 - INFO -   Word2Vec trained. Vocabulary: 15156
2025-12-17 15:31:51,029 - INFO -   Embedding coverage: 99.99% (15156/15157)
2025-12-17 15:31:51,029 - INFO - Step 4: Converting to sequences...
2025-12-17 15:31:51,288 - INFO -   X_train shape: (15961, 60), y_train shape: (15961, 6)
2025-12-17 15:31:51,292 - INFO -   Class weights: {0: 0.5706063206063207, 1: 0.497599451303155, 2: 2.0510151631971216, 3: 1.234416086620263, 4: 1.3761855492326263, 5: 4.683392018779343}
2025-12-17 15:31:51,292 - INFO - Step 5: Building model...
2025-12-17 15:31:51,435 - INFO -   Model built: LSTM, Parameters: 850,322
2025-12-17 15:31:51,435 - INFO - Step 6: Training LSTM model...
2025-12-17 15:49:04,790 - INFO -   Training completed in 1033.35 seconds
2025-12-17 15:49:04,791 - INFO - Step 7: Evaluating...
2025-12-17 15:49:05,630 - INFO - ================================================================================
2025-12-17 15:49:05,630 - INFO - COMPLETED: lstm_word2vec_baseline
2025-12-17 15:49:05,630 - INFO -   Val Accuracy: 0.2162 (21.62%)
2025-12-17 15:49:05,630 - INFO -   Macro F1: 0.1866
2025-12-17 15:49:05,630 - INFO -   Training Time: 1033.35s
2025-12-17 15:49:05,630 - INFO - ================================================================================
2025-12-17 15:49:05,639 - INFO - 

EXPERIMENT 4/7: gru_word2vec_baseline
2025-12-17 15:49:05,639 - INFO - ================================================================================
2025-12-17 15:49:05,639 - INFO - EXPERIMENT: gru_word2vec_baseline
2025-12-17 15:49:05,639 - INFO - ================================================================================
2025-12-17 15:49:05,659 - INFO - GPUs Available: 0
2025-12-17 15:49:05,660 - INFO - Step 1: Loading data...
2025-12-17 15:49:05,690 - INFO -   Train: 16000, Val: 2000
2025-12-17 15:49:05,690 - INFO - Step 2: Preprocessing (full_pipeline exact method)...
2025-12-17 15:49:08,138 - INFO -   Removed 34 duplicates
2025-12-17 15:49:08,451 - INFO -   Removed 2 duplicates
2025-12-17 15:49:08,455 - WARNING -   Data leakage: 5 overlapping texts found, removing from train
2025-12-17 15:49:08,457 - INFO - Step 3: Creating WORD2VEC embeddings...
2025-12-17 15:49:08,763 - INFO -   Tokenizer created. Vocabulary size: 15157
2025-12-17 15:49:08,763 - INFO -   Training Word2Vec embeddings...
2025-12-17 15:49:08,793 - INFO - collecting all words and their counts
2025-12-17 15:49:08,793 - INFO - PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
2025-12-17 15:49:08,824 - INFO - PROGRESS: at sentence #10000, processed 195943 words, keeping 11936 word types
2025-12-17 15:49:08,843 - INFO - collected 15156 word types from a corpus of 310407 raw words and 15961 sentences
2025-12-17 15:49:08,843 - INFO - Creating a fresh vocabulary
2025-12-17 15:49:08,888 - INFO - Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 15156 unique words (100.00% of original 15156, drops 0)', 'datetime': '2025-12-17T15:49:08.888216', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'prepare_vocab'}
2025-12-17 15:49:08,888 - INFO - Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 310407 word corpus (100.00% of original 310407, drops 0)', 'datetime': '2025-12-17T15:49:08.888502', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'prepare_vocab'}
2025-12-17 15:49:08,954 - INFO - deleting the raw counts dictionary of 15156 items
2025-12-17 15:49:08,954 - INFO - sample=0.001 downsamples 48 most-common words
2025-12-17 15:49:08,955 - INFO - Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 209815.12000829354 word corpus (67.6%% of prior 310407)', 'datetime': '2025-12-17T15:49:08.955034', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'prepare_vocab'}
2025-12-17 15:49:09,069 - INFO - estimated required memory for 15156 words and 50 dimensions: 13640400 bytes
2025-12-17 15:49:09,070 - INFO - resetting layer weights
2025-12-17 15:49:09,075 - INFO - Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-12-17T15:49:09.075609', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'build_vocab'}
2025-12-17 15:49:09,075 - INFO - Word2Vec lifecycle event {'msg': 'training model with 4 workers on 15156 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-12-17T15:49:09.075853', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'train'}
2025-12-17 15:49:09,236 - INFO - EPOCH 0: training on 310407 raw words (209778 effective words) took 0.2s, 1353262 effective words/s
2025-12-17 15:49:09,378 - INFO - EPOCH 1: training on 310407 raw words (209723 effective words) took 0.1s, 1520860 effective words/s
2025-12-17 15:49:09,529 - INFO - EPOCH 2: training on 310407 raw words (210013 effective words) took 0.1s, 1436294 effective words/s
2025-12-17 15:49:09,693 - INFO - EPOCH 3: training on 310407 raw words (209845 effective words) took 0.2s, 1320622 effective words/s
2025-12-17 15:49:09,866 - INFO - EPOCH 4: training on 310407 raw words (209838 effective words) took 0.2s, 1249009 effective words/s
2025-12-17 15:49:09,866 - INFO - Word2Vec lifecycle event {'msg': 'training on 1552035 raw words (1049197 effective words) took 0.8s, 1327449 effective words/s', 'datetime': '2025-12-17T15:49:09.866363', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'train'}
2025-12-17 15:49:09,866 - INFO - Word2Vec lifecycle event {'params': 'Word2Vec<vocab=15156, vector_size=50, alpha=0.025>', 'datetime': '2025-12-17T15:49:09.866518', 'gensim': '4.4.0', 'python': '3.9.21 (main, Aug 19 2025, 00:00:00) \n[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]', 'platform': 'Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34', 'event': 'created'}
2025-12-17 15:49:09,866 - INFO -   Word2Vec trained. Vocabulary: 15156
2025-12-17 15:49:09,913 - INFO -   Embedding coverage: 99.99% (15156/15157)
2025-12-17 15:49:09,913 - INFO - Step 4: Converting to sequences...
2025-12-17 15:49:10,467 - INFO -   X_train shape: (15961, 60), y_train shape: (15961, 6)
2025-12-17 15:49:10,471 - INFO -   Class weights: {0: 0.5706063206063207, 1: 0.497599451303155, 2: 2.0510151631971216, 3: 1.234416086620263, 4: 1.3761855492326263, 5: 4.683392018779343}
2025-12-17 15:49:10,471 - INFO - Step 5: Building model...
2025-12-17 15:49:10,619 - INFO -   Model built: GRU, Parameters: 827,794
2025-12-17 15:49:10,619 - INFO - Step 6: Training GRU model...
2025-12-17 15:57:46,110 - INFO -   Training completed in 515.49 seconds
2025-12-17 15:57:46,111 - INFO - Step 7: Evaluating...
2025-12-17 15:57:46,930 - INFO - ================================================================================
2025-12-17 15:57:46,930 - INFO - COMPLETED: gru_word2vec_baseline
2025-12-17 15:57:46,930 - INFO -   Val Accuracy: 0.2753 (27.53%)
2025-12-17 15:57:46,930 - INFO -   Macro F1: 0.0777
2025-12-17 15:57:46,930 - INFO -   Training Time: 515.49s
2025-12-17 15:57:46,930 - INFO - ================================================================================
2025-12-17 15:57:46,944 - INFO - 

EXPERIMENT 5/7: bilstm_glove50
2025-12-17 15:57:46,944 - INFO - ================================================================================
2025-12-17 15:57:46,944 - INFO - EXPERIMENT: bilstm_glove50
2025-12-17 15:57:46,944 - INFO - ================================================================================
2025-12-17 15:57:46,961 - INFO - GPUs Available: 0
2025-12-17 15:57:46,963 - INFO - Step 1: Loading data...
2025-12-17 15:57:46,994 - INFO -   Train: 16000, Val: 2000
2025-12-17 15:57:46,994 - INFO - Step 2: Preprocessing (full_pipeline exact method)...
2025-12-17 15:57:49,433 - INFO -   Removed 34 duplicates
2025-12-17 15:57:49,735 - INFO -   Removed 2 duplicates
2025-12-17 15:57:49,738 - WARNING -   Data leakage: 5 overlapping texts found, removing from train
2025-12-17 15:57:49,740 - INFO - Step 3: Creating GLOVE embeddings...
2025-12-17 15:57:50,020 - INFO -   Tokenizer created. Vocabulary size: 15157
2025-12-17 15:57:50,021 - INFO -   Loading GloVe from /home/lab/rabanof/Emotion_Detection_DL/glove/glove.6B.50d.txt...
2025-12-17 15:57:53,887 - INFO -   Loaded 400000 word vectors
2025-12-17 15:57:53,934 - INFO -   Embedding coverage: 93.68% (14199/15157)
2025-12-17 15:57:53,935 - INFO - Step 4: Converting to sequences...
2025-12-17 15:57:54,197 - INFO -   X_train shape: (15961, 60), y_train shape: (15961, 6)
2025-12-17 15:57:54,202 - INFO -   Class weights: {0: 0.5706063206063207, 1: 0.497599451303155, 2: 2.0510151631971216, 3: 1.234416086620263, 4: 1.3761855492326263, 5: 4.683392018779343}
2025-12-17 15:57:54,202 - INFO - Step 5: Building model...
2025-12-17 15:57:54,597 - INFO -   Model built: BILSTM, Parameters: 942,738
2025-12-17 15:57:54,597 - INFO - Step 6: Training BILSTM model...
2025-12-17 16:14:19,048 - INFO -   Training completed in 984.45 seconds
2025-12-17 16:14:19,049 - INFO - Step 7: Evaluating...
2025-12-17 16:14:20,449 - INFO - ================================================================================
2025-12-17 16:14:20,449 - INFO - COMPLETED: bilstm_glove50
2025-12-17 16:14:20,449 - INFO -   Val Accuracy: 0.8684 (86.84%)
2025-12-17 16:14:20,449 - INFO -   Macro F1: 0.8429
2025-12-17 16:14:20,449 - INFO -   Training Time: 984.45s
2025-12-17 16:14:20,450 - INFO - ================================================================================
2025-12-17 16:14:20,571 - INFO - 

EXPERIMENT 6/7: lstm_glove50_256units
2025-12-17 16:14:20,573 - INFO - ================================================================================
2025-12-17 16:14:20,573 - INFO - EXPERIMENT: lstm_glove50_256units
2025-12-17 16:14:20,573 - INFO - ================================================================================
2025-12-17 16:14:20,595 - INFO - GPUs Available: 0
2025-12-17 16:14:20,596 - INFO - Step 1: Loading data...
2025-12-17 16:14:20,625 - INFO -   Train: 16000, Val: 2000
2025-12-17 16:14:20,625 - INFO - Step 2: Preprocessing (full_pipeline exact method)...
2025-12-17 16:14:23,048 - INFO -   Removed 34 duplicates
2025-12-17 16:14:23,348 - INFO -   Removed 2 duplicates
2025-12-17 16:14:23,351 - WARNING -   Data leakage: 5 overlapping texts found, removing from train
2025-12-17 16:14:23,353 - INFO - Step 3: Creating GLOVE embeddings...
2025-12-17 16:14:23,644 - INFO -   Tokenizer created. Vocabulary size: 15157
2025-12-17 16:14:23,645 - INFO -   Loading GloVe from /home/lab/rabanof/Emotion_Detection_DL/glove/glove.6B.50d.txt...
2025-12-17 16:14:27,506 - INFO -   Loaded 400000 word vectors
2025-12-17 16:14:27,545 - INFO -   Embedding coverage: 93.68% (14199/15157)
2025-12-17 16:14:27,545 - INFO - Step 4: Converting to sequences...
2025-12-17 16:14:27,808 - INFO -   X_train shape: (15961, 60), y_train shape: (15961, 6)
2025-12-17 16:14:27,813 - INFO -   Class weights: {0: 0.5706063206063207, 1: 0.497599451303155, 2: 2.0510151631971216, 3: 1.234416086620263, 4: 1.3761855492326263, 5: 4.683392018779343}
2025-12-17 16:14:27,813 - INFO - Step 5: Building model...
2025-12-17 16:14:28,081 - INFO -   Model built: LSTM, Parameters: 1,073,810
2025-12-17 16:14:28,081 - INFO - Step 6: Training LSTM model...
2025-12-17 16:30:35,525 - INFO -   Training completed in 967.44 seconds
2025-12-17 16:30:35,525 - INFO - Step 7: Evaluating...
2025-12-17 16:30:36,944 - INFO - ================================================================================
2025-12-17 16:30:36,944 - INFO - COMPLETED: lstm_glove50_256units
2025-12-17 16:30:36,944 - INFO -   Val Accuracy: 0.3514 (35.14%)
2025-12-17 16:30:36,944 - INFO -   Macro F1: 0.0868
2025-12-17 16:30:36,944 - INFO -   Training Time: 967.44s
2025-12-17 16:30:36,944 - INFO - ================================================================================
2025-12-17 16:30:37,067 - INFO - 

EXPERIMENT 7/7: lstm_glove50_dropout05
2025-12-17 16:30:37,068 - INFO - ================================================================================
2025-12-17 16:30:37,070 - INFO - EXPERIMENT: lstm_glove50_dropout05
2025-12-17 16:30:37,070 - INFO - ================================================================================
2025-12-17 16:30:37,086 - INFO - GPUs Available: 0
2025-12-17 16:30:37,087 - INFO - Step 1: Loading data...
2025-12-17 16:30:37,119 - INFO -   Train: 16000, Val: 2000
2025-12-17 16:30:37,119 - INFO - Step 2: Preprocessing (full_pipeline exact method)...
2025-12-17 16:30:39,548 - INFO -   Removed 34 duplicates
2025-12-17 16:30:39,851 - INFO -   Removed 2 duplicates
2025-12-17 16:30:39,854 - WARNING -   Data leakage: 5 overlapping texts found, removing from train
2025-12-17 16:30:39,857 - INFO - Step 3: Creating GLOVE embeddings...
2025-12-17 16:30:40,139 - INFO -   Tokenizer created. Vocabulary size: 15157
2025-12-17 16:30:40,139 - INFO -   Loading GloVe from /home/lab/rabanof/Emotion_Detection_DL/glove/glove.6B.50d.txt...
2025-12-17 16:30:43,944 - INFO -   Loaded 400000 word vectors
2025-12-17 16:30:43,965 - INFO -   Embedding coverage: 93.68% (14199/15157)
2025-12-17 16:30:43,965 - INFO - Step 4: Converting to sequences...
2025-12-17 16:30:44,225 - INFO -   X_train shape: (15961, 60), y_train shape: (15961, 6)
2025-12-17 16:30:44,228 - INFO -   Class weights: {0: 0.5706063206063207, 1: 0.497599451303155, 2: 2.0510151631971216, 3: 1.234416086620263, 4: 1.3761855492326263, 5: 4.683392018779343}
2025-12-17 16:30:44,229 - INFO - Step 5: Building model...
2025-12-17 16:30:44,377 - INFO -   Model built: LSTM, Parameters: 850,322
2025-12-17 16:30:44,377 - INFO - Step 6: Training LSTM model...
2025-12-17 16:48:16,738 - INFO -   Training completed in 1052.36 seconds
2025-12-17 16:48:16,738 - INFO - Step 7: Evaluating...
2025-12-17 16:48:17,574 - INFO - ================================================================================
2025-12-17 16:48:17,574 - INFO - COMPLETED: lstm_glove50_dropout05
2025-12-17 16:48:17,574 - INFO -   Val Accuracy: 0.7943 (79.43%)
2025-12-17 16:48:17,574 - INFO -   Macro F1: 0.7756
2025-12-17 16:48:17,574 - INFO -   Training Time: 1052.36s
2025-12-17 16:48:17,574 - INFO - ================================================================================
2025-12-17 16:48:17,713 - INFO - 

================================================================================
2025-12-17 16:48:17,715 - INFO - ALL EXPERIMENTS COMPLETED - COMPARISON TABLE
2025-12-17 16:48:17,715 - INFO - ================================================================================
2025-12-17 16:48:17,738 - INFO - 
Comparison saved to: results/all_experiments_comparison.csv
2025-12-17 16:48:17,738 - INFO - 
================================================================================
2025-12-17 16:48:17,738 - INFO - BEST MODEL
2025-12-17 16:48:17,738 - INFO - ================================================================================
2025-12-17 16:48:17,738 - INFO - Experiment: bilstm_glove50
2025-12-17 16:48:17,738 - INFO - Model: BILSTM
2025-12-17 16:48:17,738 - INFO - Embedding: GLOVE 50d
2025-12-17 16:48:17,738 - INFO - Accuracy: 0.8684 (86.84%)
2025-12-17 16:48:17,738 - INFO - Macro F1: 0.8429
2025-12-17 16:48:17,738 - INFO - ================================================================================
2025-12-17 16:48:17,738 - INFO - 
All experiments completed successfully!
