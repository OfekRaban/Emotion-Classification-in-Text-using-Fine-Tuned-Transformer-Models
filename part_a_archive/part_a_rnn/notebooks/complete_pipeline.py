{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Detection Pipeline\n",
    "## Professional Deep Learning System with Full EDA and Features\n",
    "## Ofek Raban Ron Gabay\n",
    "\n",
    "**This notebook includes:**\n",
    "- EDA and preprocessing \n",
    "-  All  classes and functions (self-contained)\n",
    "-  Advanced features: ablation studies, detailed logging, model comparison\n",
    "-  Complete experiment tracking and reproducibility\n",
    "-  Comprehensive visualizations and metrics\n",
    "\n",
    "### Emotion Classes:\n",
    "0. Sadness  | 1. Joy  | 2. Love  | 3. Anger  | 4. Fear  | 5. Surprise "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Section 1: Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-16 14:07:05,322 - INFO - Random seed set to: 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All imports successful!\n",
      "TensorFlow version: 2.13.0\n",
      "GPU Available: []\n",
      "Random Seed: 42\n"
     ]
    }
   ],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import logging\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from collections import Counter\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, \n",
    "    precision_recall_fscore_support, accuracy_score\n",
    ")\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, ModelCheckpoint, ReduceLROnPlateau,\n",
    "    TensorBoard, CSVLogger, Callback\n",
    ")\n",
    "\n",
    "# Embeddings\n",
    "from gensim.models import Word2Vec\n",
    "import emoji\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('pipeline.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(f\"Random seed set to: {RANDOM_SEED}\")\n",
    "print(\" All imports successful!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"Random Seed: {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Section 2: Advanced Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-16 14:07:27,274 - INFO - ================================================================================\n",
      "2025-12-16 14:07:27,275 - INFO - EXPERIMENT CONFIGURATION\n",
      "2025-12-16 14:07:27,276 - INFO - ================================================================================\n",
      "2025-12-16 14:07:27,277 - INFO - Experiment: ultimate_emotion_detection\n",
      "2025-12-16 14:07:27,278 - INFO - Model: LSTM\n",
      "2025-12-16 14:07:27,279 - INFO - RNN Units: 128, Layers: 1\n",
      "2025-12-16 14:07:27,280 - INFO - Embedding: GLOVE, Dim: 100, Trainable: False\n",
      "2025-12-16 14:07:27,282 - INFO - Ablation - Aggressive Norm: True\n",
      "2025-12-16 14:07:27,283 - INFO - Ablation - Elongation Norm: True\n",
      "2025-12-16 14:07:27,284 - INFO - Random Seed: 42\n",
      "2025-12-16 14:07:27,285 - INFO - ================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Configuration created!\n",
      " Experiment: ultimate_emotion_detection\n",
      " Model: LSTM (128 units, 1 layers)\n",
      " Embedding: GLOVE 100d (Trainable: False)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"Complete configuration with ablation flags and advanced options.\"\"\"\n",
    "    \n",
    "    # ========== Experiment Info ==========\n",
    "    experiment_name: str = \"ultimate_emotion_detection\"\n",
    "    random_seed: int = RANDOM_SEED\n",
    "    \n",
    "    # ========== Data Paths ==========\n",
    "    train_path: str = \"/home/lab/rabanof/projects/Emotion_Detection_DL/data/raw/train.csv\"\n",
    "    val_path: str = \"/home/lab/rabanof/projects/Emotion_Detection_DL/data/raw/validation.csv\"\n",
    "    glove_path: str = \"/home/lab/rabanof/Emotion_Detection_DL/glove/glove.6B.100d.txt\"\n",
    "    \n",
    "    # ========== Data Parameters ==========\n",
    "    max_len: int = 60\n",
    "    max_words: int = 20000\n",
    "    text_column: str = \"text\"\n",
    "    label_column: str = \"label\"\n",
    "    num_classes: int = 6\n",
    "    \n",
    "    # ========== Preprocessing Ablation Flags ==========\n",
    "    enable_aggressive_normalization: bool = True  # Slang, typos, etc.\n",
    "    enable_elongation_normalization: bool = True  # sooo -> soo\n",
    "    enable_contraction_expansion: bool = True     # don't -> do not\n",
    "    \n",
    "    # ========== Embedding Configuration ==========\n",
    "    embedding_type: str = \"glove\"  # 'glove' or 'word2vec'\n",
    "    embedding_dim: int = 100\n",
    "    trainable_embeddings: bool = False  # Ablation: True vs False\n",
    "    oov_token: str = \"<UNK>\"\n",
    "    oov_init_std: float = 0.1  # Std dev for OOV random initialization\n",
    "    \n",
    "    # ========== Model Architecture ==========\n",
    "    model_type: str = \"lstm\"  # 'lstm', 'gru', 'bilstm', 'bigru'\n",
    "    rnn_units: int = 128\n",
    "    num_rnn_layers: int = 1  # Number of recurrent layers\n",
    "    \n",
    "    # ========== Regularization ==========\n",
    "    spatial_dropout: float = 0.2  # After embedding\n",
    "    dropout: float = 0.2          # After RNN\n",
    "    recurrent_dropout: float = 0.0  # Within RNN (set to 0 for GPU efficiency)\n",
    "    use_layer_norm: bool = False   # Layer normalization after RNN\n",
    "    \n",
    "    # ========== Training ==========\n",
    "    epochs: int = 50\n",
    "    batch_size: int = 32\n",
    "    learning_rate: float = 0.001\n",
    "    use_class_weights: bool = True\n",
    "    \n",
    "    # ========== Callbacks ==========\n",
    "    early_stopping: bool = True\n",
    "    patience: int = 5\n",
    "    reduce_lr: bool = True\n",
    "    lr_factor: float = 0.5\n",
    "    lr_patience: int = 3\n",
    "    min_lr: float = 1e-7\n",
    "    \n",
    "    # ========== Logging & Saving ==========\n",
    "    verbose: int = 1\n",
    "    save_tokenizer: bool = True\n",
    "    save_config: bool = True\n",
    "    save_embedding_matrix: bool = True\n",
    "    \n",
    "    # ========== Directories ==========\n",
    "    save_dir: str = \"saved_models\"\n",
    "    log_dir: str = \"logs\"\n",
    "    result_dir: str = \"results\"\n",
    "    config_dir: str = \"configs\"\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"Convert to dictionary.\"\"\"\n",
    "        return asdict(self)\n",
    "    \n",
    "    def save_to_json(self, filepath: str):\n",
    "        \"\"\"Save configuration to JSON file.\"\"\"\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(self.to_dict(), f, indent=2)\n",
    "        logger.info(f\"Configuration saved to {filepath}\")\n",
    "\n",
    "# Create default configuration\n",
    "config = ExperimentConfig()\n",
    "\n",
    "# Log configuration\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(\"EXPERIMENT CONFIGURATION\")\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(f\"Experiment: {config.experiment_name}\")\n",
    "logger.info(f\"Model: {config.model_type.upper()}\")\n",
    "logger.info(f\"RNN Units: {config.rnn_units}, Layers: {config.num_rnn_layers}\")\n",
    "logger.info(f\"Embedding: {config.embedding_type.upper()}, Dim: {config.embedding_dim}, Trainable: {config.trainable_embeddings}\")\n",
    "logger.info(f\"Ablation - Aggressive Norm: {config.enable_aggressive_normalization}\")\n",
    "logger.info(f\"Ablation - Elongation Norm: {config.enable_elongation_normalization}\")\n",
    "logger.info(f\"Random Seed: {config.random_seed}\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "print(\"\\n Configuration created!\")\n",
    "print(f\" Experiment: {config.experiment_name}\")\n",
    "print(f\" Model: {config.model_type.upper()} ({config.rnn_units} units, {config.num_rnn_layers} layers)\")\n",
    "print(f\" Embedding: {config.embedding_type.upper()} {config.embedding_dim}d (Trainable: {config.trainable_embeddings})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  How to Use This Notebook\n",
    "\n",
    "### Quick Start:\n",
    "1. **Run All Cells**: Execute cells in order from top to bottom\n",
    "2. **Modify Config**: Change parameters in Section 2 to experiment\n",
    "3. **Re-run Sections**: After changing config, re-run relevant sections\n",
    "\n",
    "### Sections Overview:\n",
    "- **Sections 1-2**: Setup and configuration (always run first)\n",
    "- **Sections 3-7**: Professional classes (run once)\n",
    "- **Sections 8-13**: Data loading and EDA (run once)\n",
    "- **Sections 14**: Preprocessing (run once per config change)\n",
    "- **Sections 15-16**: Tokenization and embeddings (re-run if embedding changes)\n",
    "- **Sections 17-19**: Model building and training (re-run for each experiment)\n",
    "- **Sections 20-23**: Evaluation (re-run after each training)\n",
    "- **Sections 24-25**: Model comparison (run to compare experiments)\n",
    "- **Sections 26-27**: Predictions and testing (run anytime after training)\n",
    "- **Section 28**: Final summary\n",
    "\n",
    "### Hyperparameter Experiments:\n",
    "To compare different configurations:\n",
    "```python\n",
    "# Experiment 1: LSTM with GloVe\n",
    "config.model_type = 'lstm'\n",
    "config.embedding_type = 'glove'\n",
    "config.rnn_units = 128\n",
    "# Run sections 15-23\n",
    "\n",
    "# Experiment 2: GRU with Word2Vec  \n",
    "config.model_type = 'gru'\n",
    "config.embedding_type = 'word2vec'\n",
    "config.rnn_units = 128\n",
    "config.experiment_name = 'gru_word2vec'\n",
    "# Run sections 15-23\n",
    "# Add to comparer: comparer.add_experiment(...)\n",
    "\n",
    "# View comparison\n",
    "comparer.create_comparison_table()\n",
    "```\n",
    "\n",
    "### Ablation Studies:\n",
    "Test impact of preprocessing:\n",
    "```python\n",
    "config.enable_elongation_normalization = False\n",
    "config.enable_contraction_expansion = False\n",
    "# Re-run from Section 14\n",
    "```\n",
    "\n",
    "### Key Configuration Parameters:\n",
    "\n",
    "**Model Architecture:**\n",
    "- `model_type`: 'lstm', 'gru', 'bilstm', 'bigru'\n",
    "- `rnn_units`: 64, 128, 256\n",
    "- `num_rnn_layers`: 1, 2, 3\n",
    "- `use_layer_norm`: True/False\n",
    "\n",
    "**Embeddings:**\n",
    "- `embedding_type`: 'glove', 'word2vec'\n",
    "- `embedding_dim`: 50, 100, 200, 300\n",
    "- `trainable_embeddings`: True/False\n",
    "\n",
    "**Regularization:**\n",
    "- `dropout`: 0.0 to 0.5\n",
    "- `spatial_dropout`: 0.0 to 0.5\n",
    "- `use_class_weights`: True/False\n",
    "\n",
    "**Training:**\n",
    "- `epochs`: 20-100\n",
    "- `batch_size`: 16, 32, 64\n",
    "- `learning_rate`: 0.0001 to 0.01\n",
    "- `patience`: 3-10 (early stopping)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#  HYPERPARAMETER EXPERIMENTATION\n",
    "\n",
    "**To compare different models:**\n",
    "1. Modify the configuration in Section 2 (change model_type, rnn_units, embedding_type, etc.)\n",
    "2. Re-run all cells from Section 15 onwards\n",
    "3. Use the ModelComparer below to compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Advanced Text Preprocessor with Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Advanced Text Preprocessor class created!\n"
     ]
    }
   ],
   "source": [
    "class AdvancedTextPreprocessor:\n",
    "    \n",
    "    # Advanced text preprocessing with ablation flags and statistics logging.\n",
    "    \n",
    "    \n",
    "    def __init__(self, config: ExperimentConfig):\n",
    "        self.config = config\n",
    "\n",
    "        # specific contractions by examaples we saw in the data\n",
    "        self.specific_contractions = {\n",
    "            \"didnt\": \"did not\", \"dont\": \"do not\", \"cant\": \"cannot\",\n",
    "            \"wont\": \"will not\", \"wouldnt\": \"would not\", \"shouldnt\": \"should not\",\n",
    "            \"couldnt\": \"could not\", \"im\": \"i am\", \"ive\": \"i have\",\n",
    "            \"id\": \"i would\", \"ill\": \"i will\", \"hadnt\": \"had not\",\n",
    "            \"youve\": \"you have\", \"werent\": \"were not\", \"theyve\": \"they have\",\n",
    "            \"theyll\": \"they will\", \"itll\": \"it will\", \"couldve\": \"could have\",\n",
    "            \"shouldve\": \"should have\", \"wouldve\": \"would have\", \"hadn\": \"had not\"\n",
    "        }\n",
    "        # General contraction patterns\n",
    "        self.general_contractions = {\n",
    "            \"n't\": \" not\", \"'re\": \" are\", \"'s\": \" is\",\n",
    "            \"'d\": \" would\", \"'ll\": \" will\", \"'t\": \" not\",\n",
    "            \"'ve\": \" have\", \"'m\": \" am\"\n",
    "        }\n",
    "        \n",
    "        # slang and typo corrections\n",
    "        self.slang_corrections = {\n",
    "            \"idk\": \"i do not know\", \"yknow\": \"you know\",\n",
    "            \"becuz\": \"because\", \"alittle\": \"a little\", \"incase\": \"in case\"\n",
    "        }\n",
    "        # corrections for common typos\n",
    "        self.typo_corrections = {\n",
    "            \"vunerable\": \"vulnerable\", \"percieve\": \"perceive\",\n",
    "            \"definetly\": \"definitely\", \"writting\": \"writing\"\n",
    "        }\n",
    "        \n",
    "        # Statistics\n",
    "        self.stats = {\n",
    "            'tokens_before': 0,\n",
    "            'tokens_after': 0,\n",
    "            'texts_modified': 0,\n",
    "            'total_texts': 0\n",
    "        }\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "\n",
    "        # Apply comprehensive text cleaning with ablation flags.\n",
    "        \n",
    "        #check if text is empty\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        original_text = text\n",
    "        \n",
    "        # Lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Elongation normalization (e.g., sooo -> soo) , save the intense feeling, for 2 chars.\n",
    "        if self.config.enable_elongation_normalization:\n",
    "            text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "        \n",
    "        # Contraction expansion (e.g., don't -> do not)\n",
    "        if self.config.enable_contraction_expansion:\n",
    "            # Specific contractions first (word boundaries)\n",
    "            for key, value in self.specific_contractions.items():\n",
    "                text = re.sub(rf'\\b{re.escape(key)}\\b', value, text)   #re.sub(pattern, replacement, text)\n",
    "            \n",
    "            # General patterns\n",
    "            for key, value in self.general_contractions.items():\n",
    "                text = text.replace(key, value)\n",
    "        \n",
    "        # Aggressive normalization (slang + typos)\n",
    "        if self.config.enable_aggressive_normalization:\n",
    "            for corrections in [self.slang_corrections, self.typo_corrections]:\n",
    "                for key, value in corrections.items():\n",
    "                    text = re.sub(rf'\\b{re.escape(key)}\\b', value, text)\n",
    "        \n",
    "        # Reduce repeated punctuation (e.g., !!! -> !)\n",
    "        text = re.sub(r\"([!?.,])\\1+\", r\"\\1\", text)\n",
    "        text = re.sub(r\"\\.{2,}\", \".\", text)\n",
    "        \n",
    "        # Normalize whitespace (remove extra spaces)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # Track if modified\n",
    "        if text != original_text.lower().strip():\n",
    "            self.stats['texts_modified'] += 1\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def preprocess_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \n",
    "        # Preprocess dataframe and collect statistics.\n",
    "        \n",
    "        # save a copy to avoid modifying original\n",
    "        df = df.copy()\n",
    "        self.stats['total_texts'] = len(df)\n",
    "        \n",
    "        # log start\n",
    "        logger.info(f\"Preprocessing {len(df)} samples...\")\n",
    "        \n",
    "        # Count tokens before\n",
    "        tokens_before = df[self.config.text_column].str.split().str.len().sum()\n",
    "        self.stats['tokens_before'] = tokens_before\n",
    "        \n",
    "        # Apply cleaning\n",
    "        df[self.config.text_column] = df[self.config.text_column].apply(self.clean_text)\n",
    "        \n",
    "        # Count tokens after\n",
    "        tokens_after = df[self.config.text_column].str.split().str.len().sum()\n",
    "        self.stats['tokens_after'] = tokens_after\n",
    "        \n",
    "        # Add text length column\n",
    "        df['text_len'] = df[self.config.text_column].str.split().str.len()\n",
    "        \n",
    "        # Log statistics\n",
    "        avg_before = tokens_before / len(df)\n",
    "        avg_after = tokens_after / len(df)\n",
    "        pct_modified = (self.stats['texts_modified'] / len(df)) * 100\n",
    "        \n",
    "        logger.info(f\"Preprocessing Statistics:\")\n",
    "        logger.info(f\"  Avg tokens before: {avg_before:.2f}\")\n",
    "        logger.info(f\"  Avg tokens after: {avg_after:.2f}\")\n",
    "        logger.info(f\"  Texts modified: {self.stats['texts_modified']} ({pct_modified:.1f}%)\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def remove_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Remove duplicate texts.\"\"\"\n",
    "\n",
    "        initial_len = len(df)\n",
    "        df = df.drop_duplicates(subset=[self.config.text_column], keep='first')\n",
    "        removed = initial_len - len(df)\n",
    "        if removed > 0:\n",
    "            logger.warning(f\"Removed {removed} duplicates\")\n",
    "        return df.reset_index(drop=True)\n",
    "    \n",
    "    def check_data_leakage(self, train_df: pd.DataFrame, val_df: pd.DataFrame) -> Tuple[pd.DataFrame, int]:\n",
    "        \"\"\"Check and remove overlapping texts.\"\"\"\n",
    "        train_texts = set(train_df[self.config.text_column])\n",
    "        val_texts = set(val_df[self.config.text_column])\n",
    "        overlaps = val_texts.intersection(train_texts)\n",
    "        \n",
    "        if len(overlaps) > 0:\n",
    "            logger.warning(f\"Data leakage: {len(overlaps)} overlapping texts found\")\n",
    "            val_df_clean = val_df[~val_df[self.config.text_column].isin(overlaps)].copy()\n",
    "            return val_df_clean.reset_index(drop=True), len(overlaps)\n",
    "        \"\"\"\"avoid data leakage, overlapping samples are removed from the validation set rather than the training set, \n",
    "        in order to preserve the integrity and size of the training data while ensuring a fair and unbiased evaluation on the validation set.\"\"\"\n",
    "        \n",
    "        logger.info(\"No data leakage detected\")\n",
    "        return val_df, 0\n",
    "    \n",
    "    def compute_class_weights(self, labels: np.ndarray) -> Dict[int, float]:\n",
    "        \"\"\"Compute balanced class weights.\"\"\"\n",
    "        # Calculate class weights to handle class imbalance, using sklearn utility for weighted loss.\n",
    "        classes = np.unique(labels)\n",
    "        weights = compute_class_weight('balanced', classes=classes, y=labels)\n",
    "        class_weights = dict(zip(classes, weights))\n",
    "        \n",
    "        logger.info(f\"Class weights computed: {class_weights}\")\n",
    "        return class_weights\n",
    "    \n",
    "    def log_class_distribution(self, labels: np.ndarray, emotion_map: Dict):\n",
    "        \"\"\"\n",
    "        Log class only for distribution with counts, percentages, and imbalance ratio.\n",
    "        \"\"\"\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        total = len(labels)\n",
    "        \n",
    "        logger.info(\"=\"*80)\n",
    "        logger.info(\"CLASS DISTRIBUTION\")\n",
    "        logger.info(\"=\"*80)\n",
    "        \n",
    "        distribution_data = []\n",
    "        for label, count in zip(unique, counts):\n",
    "            pct = (count / total) * 100\n",
    "            distribution_data.append({\n",
    "                'label': label,\n",
    "                'emotion': emotion_map[label],\n",
    "                'count': count,\n",
    "                'percentage': pct\n",
    "            })\n",
    "            logger.info(f\"  {emotion_map[label]:12s}: {count:5d} ({pct:5.2f}%)\")\n",
    "        \n",
    "        # Calculate imbalance ratio\n",
    "        max_count = counts.max()\n",
    "        min_count = counts.min()\n",
    "        imbalance_ratio = max_count / min_count\n",
    "        \n",
    "        logger.info(f\"\\nClass Imbalance Ratio: {imbalance_ratio:.2f}:1\")\n",
    "        logger.info(f\"(Max: {max_count}, Min: {min_count})\")\n",
    "        logger.info(\"=\"*80)\n",
    "        \n",
    "        return pd.DataFrame(distribution_data)\n",
    "\n",
    "print(\" Advanced Text Preprocessor class created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Section 4: Advanced Embedding Handler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Section 5: Advanced Model Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedEmbeddingHandler:\n",
    "    \"\"\"\n",
    "    Advanced embedding handler with GloVe/Word2Vec support and detailed analytics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: ExperimentConfig):\n",
    "        self.config = config\n",
    "        self.tokenizer = None\n",
    "        self.embedding_matrix = None\n",
    "        self.embeddings_index = {}\n",
    "        self.word2vec_model = None\n",
    "        \n",
    "        # Statistics\n",
    "        self.stats = {\n",
    "            'vocab_size': 0,\n",
    "            'coverage_count': 0,\n",
    "            'coverage_percent': 0.0,\n",
    "            'oov_count': 0,\n",
    "            'oov_percent': 0.0,\n",
    "            'oov_words': []\n",
    "        }\n",
    "    \n",
    "    def create_tokenizer(self, texts: List[str]) -> Tokenizer:\n",
    "        \"\"\"\n",
    "        Create and fit tokenizer on texts.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Creating tokenizer with max_words={self.config.max_words}...\")\n",
    "        \n",
    "        self.tokenizer = Tokenizer(\n",
    "            num_words=self.config.max_words,\n",
    "            oov_token=self.config.oov_token,\n",
    "            lower=True\n",
    "        )\n",
    "        self.tokenizer.fit_on_texts(texts)\n",
    "        \n",
    "        word_index = self.tokenizer.word_index\n",
    "        self.stats['vocab_size'] = min(len(word_index), self.config.max_words)\n",
    "        \n",
    "        logger.info(f\"Tokenizer created. Vocabulary size: {self.stats['vocab_size']}\")\n",
    "        logger.info(f\"Total unique words: {len(word_index)}\")\n",
    "        \n",
    "        return self.tokenizer\n",
    "    \n",
    "    def texts_to_sequences(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert texts to padded sequences.\n",
    "        \"\"\"\n",
    "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "        padded = pad_sequences(sequences, maxlen=self.config.max_len, padding='post', truncating='post')\n",
    "        \n",
    "        logger.info(f\"Converted {len(texts)} texts to sequences of length {self.config.max_len}\")\n",
    "        \n",
    "        return padded\n",
    "    \n",
    "    def analyze_sequence_lengths(self, texts: List[str]) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyze sequence lengths and truncation.\n",
    "        \"\"\"\n",
    "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "        lengths = [len(seq) for seq in sequences]\n",
    "        \n",
    "        truncated = sum(1 for l in lengths if l > self.config.max_len)\n",
    "        truncation_pct = (truncated / len(lengths)) * 100\n",
    "        \n",
    "        stats = {\n",
    "            'mean_length': np.mean(lengths),\n",
    "            'median_length': np.median(lengths),\n",
    "            'max_length': max(lengths),\n",
    "            'min_length': min(lengths),\n",
    "            'truncated_count': truncated,\n",
    "            'truncation_percent': truncation_pct\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Sequence Length Analysis:\")\n",
    "        logger.info(f\"  Mean: {stats['mean_length']:.2f}, Median: {stats['median_length']:.0f}\")\n",
    "        logger.info(f\"  Max: {stats['max_length']}, Min: {stats['min_length']}\")\n",
    "        logger.info(f\"  Truncated: {truncated} ({truncation_pct:.1f}%)\")\n",
    "        \n",
    "        return stats, lengths\n",
    "    \n",
    "    def load_glove_embeddings(self, glove_path: str):\n",
    "        \"\"\"\n",
    "        Load pre-trained GloVe embeddings.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Loading GloVe embeddings from {glove_path}...\")\n",
    "        \n",
    "        with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], dtype='float32')\n",
    "                self.embeddings_index[word] = vector\n",
    "        \n",
    "        logger.info(f\"Loaded {len(self.embeddings_index)} word vectors\")\n",
    "    \n",
    "    def train_word2vec(self, texts: List[str], min_count: int = 1):\n",
    "        \"\"\"\n",
    "        Train Word2Vec embeddings on the corpus.\n",
    "        \"\"\"\n",
    "        logger.info(\"Training Word2Vec embeddings...\")\n",
    "        \n",
    "        # Tokenize texts into word lists\n",
    "        tokenized_texts = [text.split() for text in texts]\n",
    "        \n",
    "        # Train Word2Vec\n",
    "        self.word2vec_model = Word2Vec(\n",
    "            sentences=tokenized_texts,\n",
    "            vector_size=self.config.embedding_dim,\n",
    "            window=5,\n",
    "            min_count=min_count,\n",
    "            workers=4,\n",
    "            seed=self.config.random_seed\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Word2Vec trained on {len(texts)} texts\")\n",
    "        logger.info(f\"Vocabulary size: {len(self.word2vec_model.wv)}\")\n",
    "    \n",
    "    def create_embedding_matrix(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create embedding matrix from GloVe or Word2Vec.\n",
    "        \"\"\"\n",
    "        if self.tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer must be created first\")\n",
    "        \n",
    "        word_index = self.tokenizer.word_index\n",
    "        vocab_size = min(len(word_index) + 1, self.config.max_words + 1)\n",
    "        \n",
    "        # Initialize with small random values\n",
    "        self.embedding_matrix = np.random.normal(\n",
    "            0, self.config.oov_init_std, \n",
    "            (vocab_size, self.config.embedding_dim)\n",
    "        )\n",
    "        \n",
    "        # Set padding vector to zeros\n",
    "        self.embedding_matrix[0] = np.zeros(self.config.embedding_dim)\n",
    "        \n",
    "        # Fill with pre-trained vectors\n",
    "        found_count = 0\n",
    "        oov_words = []\n",
    "        \n",
    "        for word, i in word_index.items():\n",
    "            if i >= self.config.max_words:\n",
    "                continue\n",
    "            \n",
    "            embedding_vector = None\n",
    "            \n",
    "            if self.config.embedding_type == 'glove':\n",
    "                embedding_vector = self.embeddings_index.get(word)\n",
    "            elif self.config.embedding_type == 'word2vec' and self.word2vec_model:\n",
    "                try:\n",
    "                    embedding_vector = self.word2vec_model.wv[word]\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            \n",
    "            if embedding_vector is not None:\n",
    "                self.embedding_matrix[i] = embedding_vector\n",
    "                found_count += 1\n",
    "            else:\n",
    "                oov_words.append(word)\n",
    "        \n",
    "        # Update statistics\n",
    "        self.stats['coverage_count'] = found_count\n",
    "        self.stats['coverage_percent'] = (found_count / (vocab_size - 1)) * 100\n",
    "        self.stats['oov_count'] = len(oov_words)\n",
    "        self.stats['oov_percent'] = (len(oov_words) / (vocab_size - 1)) * 100\n",
    "        self.stats['oov_words'] = oov_words[:100]  # Store first 100\n",
    "        \n",
    "        logger.info(f\"Embedding Matrix Created:\")\n",
    "        logger.info(f\"  Shape: {self.embedding_matrix.shape}\")\n",
    "        logger.info(f\"  Coverage: {found_count}/{vocab_size-1} ({self.stats['coverage_percent']:.2f}%)\")\n",
    "        logger.info(f\"  OOV: {len(oov_words)} ({self.stats['oov_percent']:.2f}%)\")\n",
    "        \n",
    "        return self.embedding_matrix\n",
    "    \n",
    "    def get_oov_rate(self, sequences: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculate OOV rate in sequences.\n",
    "        \"\"\"\n",
    "        if self.tokenizer is None:\n",
    "            return 0.0\n",
    "        \n",
    "        oov_index = self.tokenizer.word_index.get(self.config.oov_token, 1)\n",
    "        total_tokens = sequences.size\n",
    "        oov_tokens = np.sum(sequences == oov_index)\n",
    "        \n",
    "        oov_rate = (oov_tokens / total_tokens) * 100\n",
    "        return oov_rate\n",
    "    \n",
    "    def save_tokenizer(self, filepath: str):\n",
    "        \"\"\"Save tokenizer to JSON file.\"\"\"\n",
    "        if self.tokenizer is None:\n",
    "            logger.warning(\"No tokenizer to save\")\n",
    "            return\n",
    "        \n",
    "        tokenizer_json = self.tokenizer.to_json()\n",
    "        with open(filepath, 'w') as f:\n",
    "            f.write(tokenizer_json)\n",
    "        logger.info(f\"Tokenizer saved to {filepath}\")\n",
    "    \n",
    "    def save_embedding_matrix(self, filepath: str):\n",
    "        \"\"\"Save embedding matrix to numpy file.\"\"\"\n",
    "        if self.embedding_matrix is None:\n",
    "            logger.warning(\"No embedding matrix to save\")\n",
    "            return\n",
    "        \n",
    "        np.save(filepath, self.embedding_matrix)\n",
    "        logger.info(f\"Embedding matrix saved to {filepath}\")\n",
    "\n",
    "print(\" AdvancedEmbeddingHandler class created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Section 6: Results Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedModelBuilder:\n",
    "    \"\"\"\n",
    "    Advanced model builder supporting LSTM, GRU, and Bidirectional variants.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: ExperimentConfig):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "    \n",
    "    def build_model(self, embedding_matrix: np.ndarray) -> keras.Model:\n",
    "        \"\"\"\n",
    "        Build model based on configuration.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Building {self.config.model_type.upper()} model...\")\n",
    "        \n",
    "        vocab_size, embedding_dim = embedding_matrix.shape\n",
    "        \n",
    "        # Input layer\n",
    "        inputs = layers.Input(shape=(self.config.max_len,), name='input')\n",
    "        \n",
    "        # Embedding layer\n",
    "        x = layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_dim,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=self.config.max_len,\n",
    "            trainable=self.config.trainable_embeddings,\n",
    "            name='embedding'\n",
    "        )(inputs)\n",
    "        \n",
    "        # Spatial dropout after embedding\n",
    "        if self.config.spatial_dropout > 0:\n",
    "            x = layers.SpatialDropout1D(self.config.spatial_dropout, name='spatial_dropout')(x)\n",
    "        \n",
    "        # Recurrent layers\n",
    "        for layer_idx in range(self.config.num_rnn_layers):\n",
    "            return_sequences = (layer_idx < self.config.num_rnn_layers - 1)\n",
    "            \n",
    "            # Choose RNN type\n",
    "            if self.config.model_type in ['lstm', 'bilstm']:\n",
    "                rnn_layer = layers.LSTM(\n",
    "                    units=self.config.rnn_units,\n",
    "                    return_sequences=return_sequences,\n",
    "                    dropout=self.config.dropout if layer_idx == 0 else 0,\n",
    "                    recurrent_dropout=self.config.recurrent_dropout,\n",
    "                    name=f'lstm_{layer_idx+1}'\n",
    "                )\n",
    "            elif self.config.model_type in ['gru', 'bigru']:\n",
    "                rnn_layer = layers.GRU(\n",
    "                    units=self.config.rnn_units,\n",
    "                    return_sequences=return_sequences,\n",
    "                    dropout=self.config.dropout if layer_idx == 0 else 0,\n",
    "                    recurrent_dropout=self.config.recurrent_dropout,\n",
    "                    name=f'gru_{layer_idx+1}'\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown model type: {self.config.model_type}\")\n",
    "            \n",
    "            # Apply bidirectional wrapper if needed\n",
    "            if self.config.model_type in ['bilstm', 'bigru']:\n",
    "                rnn_layer = layers.Bidirectional(rnn_layer, name=f'bidirectional_{layer_idx+1}')\n",
    "            \n",
    "            x = rnn_layer(x)\n",
    "            \n",
    "            # Layer normalization if enabled\n",
    "            if self.config.use_layer_norm:\n",
    "                x = layers.LayerNormalization(name=f'layer_norm_{layer_idx+1}')(x)\n",
    "            \n",
    "            # Dropout after RNN (except for last layer)\n",
    "            if self.config.dropout > 0 and layer_idx < self.config.num_rnn_layers - 1:\n",
    "                x = layers.Dropout(self.config.dropout, name=f'dropout_{layer_idx+1}')(x)\n",
    "        \n",
    "        # Output layer\n",
    "        outputs = layers.Dense(\n",
    "            self.config.num_classes,\n",
    "            activation='softmax',\n",
    "            name='output'\n",
    "        )(x)\n",
    "        \n",
    "        # Create model\n",
    "        self.model = keras.Model(inputs=inputs, outputs=outputs, name=f'{self.config.model_type}_model')\n",
    "        \n",
    "        # Compile model\n",
    "        self.model.compile(\n",
    "            optimizer=Adam(learning_rate=self.config.learning_rate),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Model built successfully!\")\n",
    "        logger.info(f\"  Type: {self.config.model_type.upper()}\")\n",
    "        logger.info(f\"  Units: {self.config.rnn_units}, Layers: {self.config.num_rnn_layers}\")\n",
    "        logger.info(f\"  Trainable embeddings: {self.config.trainable_embeddings}\")\n",
    "        logger.info(f\"  Total parameters: {self.model.count_params():,}\")\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def get_model_summary(self) -> str:\n",
    "        \"\"\"Get model summary as string.\"\"\"\n",
    "        if self.model is None:\n",
    "            return \"No model built yet\"\n",
    "        \n",
    "        # Capture summary\n",
    "        from io import StringIO\n",
    "        stream = StringIO()\n",
    "        self.model.summary(print_fn=lambda x: stream.write(x + '\\n'))\n",
    "        return stream.getvalue()\n",
    "\n",
    "print(\" AdvancedModelBuilder class created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Section 7: Experiment Tracker and Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultsVisualizer:\n",
    "    \"\"\"\n",
    "    Comprehensive visualization class for model results and analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, emotion_map: Dict[int, str]):\n",
    "        self.emotion_map = emotion_map\n",
    "        self.emotion_names = [emotion_map[i] for i in sorted(emotion_map.keys())]\n",
    "    \n",
    "    def plot_training_history(self, history, save_path: Optional[str] = None):\n",
    "        \"\"\"Plot training and validation accuracy/loss curves.\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Accuracy plot\n",
    "        ax1.plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "        ax1.plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "        ax1.set_title('Model Accuracy Over Epochs', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('Epoch', fontsize=12)\n",
    "        ax1.set_ylabel('Accuracy', fontsize=12)\n",
    "        ax1.legend(fontsize=11)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Loss plot\n",
    "        ax2.plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "        ax2.plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "        ax2.set_title('Model Loss Over Epochs', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('Epoch', fontsize=12)\n",
    "        ax2.set_ylabel('Loss', fontsize=12)\n",
    "        ax2.legend(fontsize=11)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_confusion_matrix(self, y_true, y_pred, normalize=False, save_path: Optional[str] = None):\n",
    "        \"\"\"Plot confusion matrix.\"\"\"\n",
    "        # Convert one-hot to labels if needed\n",
    "        if len(y_true.shape) > 1:\n",
    "            y_true = y_true.argmax(axis=1)\n",
    "        if len(y_pred.shape) > 1:\n",
    "            y_pred = y_pred.argmax(axis=1)\n",
    "        \n",
    "        # Compute confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        if normalize:\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            fmt = '.2%'\n",
    "            title = 'Normalized Confusion Matrix'\n",
    "        else:\n",
    "            fmt = 'd'\n",
    "            title = 'Confusion Matrix'\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt=fmt, cmap='Blues', \n",
    "                    xticklabels=self.emotion_names, yticklabels=self.emotion_names,\n",
    "                    cbar_kws={'label': 'Percentage' if normalize else 'Count'})\n",
    "        plt.title(title, fontsize=14, fontweight='bold', pad=20)\n",
    "        plt.ylabel('True Label', fontsize=12)\n",
    "        plt.xlabel('Predicted Label', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_classification_report(self, y_true, y_pred, save_path: Optional[str] = None):\n",
    "        \"\"\"Plot classification report as heatmap.\"\"\"\n",
    "        # Convert one-hot to labels if needed\n",
    "        if len(y_true.shape) > 1:\n",
    "            y_true = y_true.argmax(axis=1)\n",
    "        if len(y_pred.shape) > 1:\n",
    "            y_pred = y_pred.argmax(axis=1)\n",
    "        \n",
    "        # Get metrics\n",
    "        precision, recall, f1, support = precision_recall_fscore_support(\n",
    "            y_true, y_pred, labels=list(range(len(self.emotion_names)))\n",
    "        )\n",
    "        \n",
    "        # Create dataframe\n",
    "        metrics_df = pd.DataFrame({\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-Score': f1\n",
    "        }, index=self.emotion_names)\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.heatmap(metrics_df.T, annot=True, fmt='.3f', cmap='RdYlGn', \n",
    "                    vmin=0, vmax=1, cbar_kws={'label': 'Score'})\n",
    "        plt.title('Classification Report by Emotion', fontsize=14, fontweight='bold', pad=20)\n",
    "        plt.ylabel('Metric', fontsize=12)\n",
    "        plt.xlabel('Emotion', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return metrics_df\n",
    "    \n",
    "    def plot_per_class_metrics(self, y_true, y_pred, metric='f1', save_path: Optional[str] = None):\n",
    "        \"\"\"Plot per-class metrics as bar chart.\"\"\"\n",
    "        # Convert one-hot to labels if needed\n",
    "        if len(y_true.shape) > 1:\n",
    "            y_true = y_true.argmax(axis=1)\n",
    "        if len(y_pred.shape) > 1:\n",
    "            y_pred = y_pred.argmax(axis=1)\n",
    "        \n",
    "        # Get metrics\n",
    "        precision, recall, f1, support = precision_recall_fscore_support(\n",
    "            y_true, y_pred, labels=list(range(len(self.emotion_names)))\n",
    "        )\n",
    "        \n",
    "        # Select metric\n",
    "        metric_map = {\n",
    "            'precision': ('Precision', precision),\n",
    "            'recall': ('Recall', recall),\n",
    "            'f1': ('F1-Score', f1)\n",
    "        }\n",
    "        \n",
    "        metric_name, values = metric_map.get(metric.lower(), ('F1-Score', f1))\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, len(self.emotion_names)))\n",
    "        bars = plt.bar(self.emotion_names, values, color=colors, edgecolor='black', linewidth=1.5)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.3f}',\n",
    "                    ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.title(f'Per-Class {metric_name}', fontsize=14, fontweight='bold')\n",
    "        plt.ylabel(metric_name, fontsize=12)\n",
    "        plt.xlabel('Emotion', fontsize=12)\n",
    "        plt.ylim(0, 1.1)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_label_distribution(self, labels, title='Label Distribution', save_path: Optional[str] = None):\n",
    "        \"\"\"Plot label distribution.\"\"\"\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(unique)))\n",
    "        bars = plt.bar([self.emotion_map[i] for i in unique], counts, color=colors, \n",
    "                      edgecolor='black', linewidth=1.5)\n",
    "        \n",
    "        # Add percentages\n",
    "        total = sum(counts)\n",
    "        for bar, count in zip(bars, counts):\n",
    "            height = bar.get_height()\n",
    "            pct = (count / total) * 100\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{count}\\n({pct:.1f}%)',\n",
    "                    ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.title(title, fontsize=14, fontweight='bold')\n",
    "        plt.ylabel('Count', fontsize=12)\n",
    "        plt.xlabel('Emotion', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_text_length_distribution(self, text_lengths, max_len=None, save_path: Optional[str] = None):\n",
    "        \"\"\"Plot text length distribution.\"\"\"\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.hist(text_lengths, bins=50, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "        plt.axvline(np.mean(text_lengths), color='red', linestyle='--', \n",
    "                   linewidth=2, label=f'Mean: {np.mean(text_lengths):.1f}')\n",
    "        plt.axvline(np.median(text_lengths), color='green', linestyle='--', \n",
    "                   linewidth=2, label=f'Median: {np.median(text_lengths):.0f}')\n",
    "        \n",
    "        if max_len:\n",
    "            plt.axvline(max_len, color='orange', linestyle='--', \n",
    "                       linewidth=2, label=f'MAX_LEN: {max_len}')\n",
    "        \n",
    "        plt.title('Text Length Distribution', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Text Length (words)', fontsize=12)\n",
    "        plt.ylabel('Frequency', fontsize=12)\n",
    "        plt.legend(fontsize=11)\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_word_cloud(self, texts, title='Word Cloud', save_path: Optional[str] = None):\n",
    "        \"\"\"Generate and plot word cloud.\"\"\"\n",
    "        # Combine all texts\n",
    "        combined_text = ' '.join(texts)\n",
    "        \n",
    "        # Generate word cloud\n",
    "        wordcloud = WordCloud(\n",
    "            width=800, height=400,\n",
    "            background_color='white',\n",
    "            colormap='viridis',\n",
    "            max_words=100,\n",
    "            relative_scaling=0.5,\n",
    "            min_font_size=10\n",
    "        ).generate(combined_text)\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.title(title, fontsize=14, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "print(\" ResultsVisualizer class created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Section 8: Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#  DATA LOADING AND EXPLORATION\n",
    "\n",
    "This section includes all original EDA from your full_pipeline.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentTracker(Callback):\n",
    "    \"\"\"\n",
    "    Custom Keras callback to track experiment metrics and training time.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: ExperimentConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.experiment_results = {\n",
    "            'config': config.to_dict(),\n",
    "            'training_time_per_epoch': [],\n",
    "            'total_training_time': 0,\n",
    "            'best_val_accuracy': 0,\n",
    "            'best_epoch': 0\n",
    "        }\n",
    "        self.epoch_start_time = None\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        \"\"\"Record epoch start time.\"\"\"\n",
    "        self.epoch_start_time = time.time()\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"Record epoch metrics and time.\"\"\"\n",
    "        epoch_time = time.time() - self.epoch_start_time\n",
    "        self.experiment_results['training_time_per_epoch'].append(epoch_time)\n",
    "        \n",
    "        # Track best validation accuracy\n",
    "        val_acc = logs.get('val_accuracy', 0)\n",
    "        if val_acc > self.experiment_results['best_val_accuracy']:\n",
    "            self.experiment_results['best_val_accuracy'] = val_acc\n",
    "            self.experiment_results['best_epoch'] = epoch + 1\n",
    "        \n",
    "        if self.config.verbose:\n",
    "            logger.info(f\"Epoch {epoch+1} completed in {epoch_time:.2f}s - \"\n",
    "                       f\"val_acc: {val_acc:.4f}\")\n",
    "    \n",
    "    def on_train_end(self, logs=None):\n",
    "        \"\"\"Record total training time.\"\"\"\n",
    "        self.experiment_results['total_training_time'] = sum(\n",
    "            self.experiment_results['training_time_per_epoch']\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Training completed!\")\n",
    "        logger.info(f\"  Total time: {self.experiment_results['total_training_time']:.2f}s\")\n",
    "        logger.info(f\"  Best val accuracy: {self.experiment_results['best_val_accuracy']:.4f} \"\n",
    "                   f\"at epoch {self.experiment_results['best_epoch']}\")\n",
    "    \n",
    "    def get_results(self) -> Dict:\n",
    "        \"\"\"Get experiment results.\"\"\"\n",
    "        return self.experiment_results\n",
    "    \n",
    "    def save_results(self, filepath: str):\n",
    "        \"\"\"Save results to JSON file.\"\"\"\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(self.experiment_results, f, indent=2)\n",
    "        logger.info(f\"Experiment results saved to {filepath}\")\n",
    "\n",
    "\n",
    "class ModelComparer:\n",
    "    \"\"\"\n",
    "    Compare multiple model configurations and results.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, emotion_map: Dict[int, str]):\n",
    "        self.emotion_map = emotion_map\n",
    "        self.experiments = []\n",
    "    \n",
    "    def add_experiment(self, name: str, config: ExperimentConfig, history, \n",
    "                      metrics: Dict, predictions=None, y_true=None):\n",
    "        \"\"\"Add experiment results for comparison.\"\"\"\n",
    "        experiment = {\n",
    "            'name': name,\n",
    "            'config': config.to_dict(),\n",
    "            'history': history.history if hasattr(history, 'history') else history,\n",
    "            'metrics': metrics,\n",
    "            'predictions': predictions,\n",
    "            'y_true': y_true\n",
    "        }\n",
    "        self.experiments.append(experiment)\n",
    "        logger.info(f\"Added experiment: {name}\")\n",
    "    \n",
    "    def create_comparison_table(self) -> pd.DataFrame:\n",
    "        \"\"\"Create comparison table of all experiments.\"\"\"\n",
    "        comparison_data = []\n",
    "        \n",
    "        for exp in self.experiments:\n",
    "            config = exp['config']\n",
    "            metrics = exp['metrics']\n",
    "            \n",
    "            comparison_data.append({\n",
    "                'Experiment': exp['name'],\n",
    "                'Model': config['model_type'].upper(),\n",
    "                'RNN Units': config['rnn_units'],\n",
    "                'Layers': config['num_rnn_layers'],\n",
    "                'Embedding': config['embedding_type'].upper(),\n",
    "                'Embed Dim': config['embedding_dim'],\n",
    "                'Trainable Emb': config['trainable_embeddings'],\n",
    "                'Dropout': config['dropout'],\n",
    "                'Val Accuracy': metrics.get('val_accuracy', 0),\n",
    "                'Val Loss': metrics.get('val_loss', 0),\n",
    "                'Macro F1': metrics.get('macro_f1', 0),\n",
    "                'Training Time': metrics.get('training_time', 0)\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(comparison_data)\n",
    "        return df.sort_values('Val Accuracy', ascending=False)\n",
    "    \n",
    "    def plot_comparison(self, metric='val_accuracy', save_path: Optional[str] = None):\n",
    "        \"\"\"Plot comparison of experiments.\"\"\"\n",
    "        if not self.experiments:\n",
    "            print(\"No experiments to compare\")\n",
    "            return\n",
    "        \n",
    "        names = [exp['name'] for exp in self.experiments]\n",
    "        values = [exp['metrics'].get(metric, 0) for exp in self.experiments]\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, len(names)))\n",
    "        bars = plt.bar(names, values, color=colors, edgecolor='black', linewidth=1.5)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.4f}',\n",
    "                    ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.title(f'Model Comparison: {metric}', fontsize=14, fontweight='bold')\n",
    "        plt.ylabel(metric, fontsize=12)\n",
    "        plt.xlabel('Experiment', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def save_comparison(self, filepath: str):\n",
    "        \"\"\"Save comparison table to CSV.\"\"\"\n",
    "        df = self.create_comparison_table()\n",
    "        df.to_csv(filepath, index=False)\n",
    "        logger.info(f\"Comparison table saved to {filepath}\")\n",
    "\n",
    "print(\" ExperimentTracker and ModelComparer classes created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Section 9: Class Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion mapping (your original mapping)\n",
    "emotion_map = {\n",
    "    0: 'sadness',\n",
    "    1: 'joy',\n",
    "    2: 'love',\n",
    "    3: 'anger',\n",
    "    4: 'fear',\n",
    "    5: 'surprise'\n",
    "}\n",
    "\n",
    "# Load datasets\n",
    "logger.info(\"Loading datasets...\")\n",
    "train_df = pd.read_csv(config.train_path)\n",
    "val_df = pd.read_csv(config.val_path)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING DATA\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Shape: {train_df.shape}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(train_df.head())\n",
    "print(f\"\\nData Info:\")\n",
    "print(train_df.info())\n",
    "print(f\"\\nMissing values:\")\n",
    "print(train_df.isnull().sum())\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(train_df['label'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VALIDATION DATA\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Shape: {val_df.shape}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(val_df.head())\n",
    "print(f\"\\nData Info:\")\n",
    "print(val_df.info())\n",
    "print(f\"\\nMissing values:\")\n",
    "print(val_df.isnull().sum())\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(val_df['label'].value_counts().sort_index())\n",
    "\n",
    "# Store original data size\n",
    "original_train_size = len(train_df)\n",
    "original_val_size = len(val_df)\n",
    "\n",
    "logger.info(f\"Loaded {original_train_size} training and {original_val_size} validation samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Section 10: Text Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize visualizer\n",
    "visualizer = ResultsVisualizer(emotion_map)\n",
    "\n",
    "# Create detailed class distribution table\n",
    "print(\"=\"*80)\n",
    "print(\"CLASS DISTRIBUTION TABLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "train_labels = train_df['label'].values\n",
    "val_labels = val_df['label'].values\n",
    "\n",
    "# Training set distribution\n",
    "train_unique, train_counts = np.unique(train_labels, return_counts=True)\n",
    "train_total = len(train_labels)\n",
    "\n",
    "train_dist = []\n",
    "for label, count in zip(train_unique, train_counts):\n",
    "    pct = (count / train_total) * 100\n",
    "    train_dist.append({\n",
    "        'Label': label,\n",
    "        'Emotion': emotion_map[label],\n",
    "        'Train Count': count,\n",
    "        'Train %': f\"{pct:.2f}%\"\n",
    "    })\n",
    "\n",
    "# Validation set distribution\n",
    "val_unique, val_counts = np.unique(val_labels, return_counts=True)\n",
    "val_total = len(val_labels)\n",
    "\n",
    "for i, (label, count) in enumerate(zip(val_unique, val_counts)):\n",
    "    pct = (count / val_total) * 100\n",
    "    train_dist[i]['Val Count'] = count\n",
    "    train_dist[i]['Val %'] = f\"{pct:.2f}%\"\n",
    "\n",
    "dist_df = pd.DataFrame(train_dist)\n",
    "print(dist_df.to_string(index=False))\n",
    "\n",
    "# Calculate imbalance ratio\n",
    "max_count = train_counts.max()\n",
    "min_count = train_counts.min()\n",
    "imbalance_ratio = max_count / min_count\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"CLASS IMBALANCE RATIO: {imbalance_ratio:.2f}:1\")\n",
    "print(f\"Most common: {emotion_map[train_unique[train_counts.argmax()]].upper()} ({max_count} samples)\")\n",
    "print(f\"Least common: {emotion_map[train_unique[train_counts.argmin()]].upper()} ({min_count} samples)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Visualize distributions\n",
    "visualizer.plot_label_distribution(train_labels, title='Training Set - Label Distribution')\n",
    "visualizer.plot_label_distribution(val_labels, title='Validation Set - Label Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Section 11: Word Clouds by Emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate text lengths (in words)\n",
    "train_df['text_len'] = train_df['text'].str.split().str.len()\n",
    "val_df['text_len'] = val_df['text'].str.split().str.len()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TEXT LENGTH STATISTICS (Before Preprocessing)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nTraining Set:\")\n",
    "print(train_df['text_len'].describe())\n",
    "print(\"\\nValidation Set:\")\n",
    "print(val_df['text_len'].describe())\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Visualize text length distribution\n",
    "visualizer.plot_text_length_distribution(\n",
    "    train_df['text_len'].values, \n",
    "    max_len=config.max_len,\n",
    "    save_path=None\n",
    ")\n",
    "\n",
    "# Text length by emotion\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AVERAGE TEXT LENGTH BY EMOTION\")\n",
    "print(\"=\"*80)\n",
    "for label in sorted(emotion_map.keys()):\n",
    "    emotion = emotion_map[label]\n",
    "    avg_len = train_df[train_df['label'] == label]['text_len'].mean()\n",
    "    print(f\"{emotion.capitalize():12s}: {avg_len:.2f} words\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Section 12: Most Common Words Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word clouds for each emotion\n",
    "print(\"Generating word clouds for each emotion...\")\n",
    "\n",
    "for label in sorted(emotion_map.keys()):\n",
    "    emotion = emotion_map[label]\n",
    "    texts = train_df[train_df['label'] == label]['text'].values\n",
    "    visualizer.plot_word_cloud(texts, title=f'Word Cloud - {emotion.capitalize()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Section 13: Check for Twitter Noise (Emojis, Hashtags, Mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common words per emotion\n",
    "print(\"=\"*80)\n",
    "print(\"TOP 10 MOST COMMON WORDS BY EMOTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for label in sorted(emotion_map.keys()):\n",
    "    emotion = emotion_map[label]\n",
    "    texts = train_df[train_df['label'] == label]['text'].values\n",
    "    \n",
    "    # Combine all texts and split into words\n",
    "    all_words = ' '.join(texts).split()\n",
    "    word_counts = Counter(all_words)\n",
    "    \n",
    "    print(f\"\\n{emotion.upper()} (Label {label}):\")\n",
    "    for word, count in word_counts.most_common(10):\n",
    "        print(f\"  {word:15s}: {count:5d}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Section 14: Text Preprocessing with Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#  PREPROCESSING AND DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Twitter-specific elements\n",
    "print(\"=\"*80)\n",
    "print(\"CHECKING FOR TWITTER NOISE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check for emojis\n",
    "def contains_emoji(text):\n",
    "    return any(char in emoji.EMOJI_DATA for char in text)\n",
    "\n",
    "# Check for hashtags\n",
    "def contains_hashtag(text):\n",
    "    return '#' in text\n",
    "\n",
    "# Check for mentions\n",
    "def contains_mention(text):\n",
    "    return '@' in text\n",
    "\n",
    "# Analyze training set\n",
    "all_texts = train_df['text'].values\n",
    "emoji_count = sum(contains_emoji(text) for text in all_texts)\n",
    "hashtag_count = sum(contains_hashtag(text) for text in all_texts)\n",
    "mention_count = sum(contains_mention(text) for text in all_texts)\n",
    "\n",
    "print(f\"\\nTexts with emojis: {emoji_count} ({emoji_count/len(all_texts)*100:.2f}%)\")\n",
    "print(f\"Texts with hashtags: {hashtag_count} ({hashtag_count/len(all_texts)*100:.2f}%)\")\n",
    "print(f\"Texts with mentions: {mention_count} ({mention_count/len(all_texts)*100:.2f}%)\")\n",
    "\n",
    "# Check for URLs\n",
    "url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "url_count = sum(1 for text in all_texts if url_pattern.search(text))\n",
    "print(f\"Texts with URLs: {url_count} ({url_count/len(all_texts)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONCLUSION: Dataset is clean - minimal Twitter noise detected\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Section 15: Tokenization and Sequence Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#  TOKENIZATION AND EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = AdvancedTextPreprocessor(config)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PREPROCESSING CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Aggressive Normalization: {config.enable_aggressive_normalization}\")\n",
    "print(f\"Elongation Normalization: {config.enable_elongation_normalization}\")\n",
    "print(f\"Contraction Expansion: {config.enable_contraction_expansion}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show before/after examples\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPROCESSING EXAMPLES (Before \u2192 After)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sample_texts = [\n",
    "    \"i didnt feel good about this sooo bad situation!!!\",\n",
    "    \"im feeling amazing today cant wait\",\n",
    "    \"i love you sooooo much!!!\",\n",
    "    \"dont be angry with me pleeease\",\n",
    "    \"im scared idk what to do\",\n",
    "    \"omg this is soooo surprising!!\"\n",
    "]\n",
    "\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "    cleaned = preprocessor.clean_text(text)\n",
    "    print(f\"\\n{i}. Original: {text}\")\n",
    "    print(f\"   Cleaned:  {cleaned}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Preprocess datasets\n",
    "print(\"\\nPreprocessing training data...\")\n",
    "train_df_clean = preprocessor.preprocess_dataframe(train_df.copy())\n",
    "\n",
    "print(\"\\nPreprocessing validation data...\")\n",
    "val_df_clean = preprocessor.preprocess_dataframe(val_df.copy())\n",
    "\n",
    "# Check for duplicates\n",
    "print(\"\\nChecking for duplicates...\")\n",
    "train_df_clean = preprocessor.remove_duplicates(train_df_clean)\n",
    "\n",
    "# Check for data leakage\n",
    "print(\"\\nChecking for data leakage between train and validation...\")\n",
    "val_df_clean, leakage_count = preprocessor.check_data_leakage(train_df_clean, val_df_clean)\n",
    "\n",
    "if leakage_count > 0:\n",
    "    print(f\"  Removed {leakage_count} overlapping texts from validation set\")\n",
    "else:\n",
    "    print(\" No data leakage detected\")\n",
    "\n",
    "# Display preprocessing statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPROCESSING STATISTICS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Training set:\")\n",
    "print(f\"  Original size: {original_train_size}\")\n",
    "print(f\"  Final size: {len(train_df_clean)}\")\n",
    "print(f\"  Texts modified: {preprocessor.stats['texts_modified']}\")\n",
    "print(f\"\\nValidation set:\")\n",
    "print(f\"  Original size: {original_val_size}\")\n",
    "print(f\"  Final size: {len(val_df_clean)}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Section 16: Load/Train Embeddings and Create Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding handler\n",
    "embedding_handler = AdvancedEmbeddingHandler(config)\n",
    "\n",
    "# Extract texts\n",
    "train_texts = train_df_clean['text'].values.tolist()\n",
    "val_texts = val_df_clean['text'].values.tolist()\n",
    "\n",
    "# Create tokenizer\n",
    "print(\"=\"*80)\n",
    "print(\"CREATING TOKENIZER\")\n",
    "print(\"=\"*80)\n",
    "tokenizer = embedding_handler.create_tokenizer(train_texts)\n",
    "\n",
    "# Analyze sequence lengths\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEQUENCE LENGTH ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "seq_stats, seq_lengths = embedding_handler.analyze_sequence_lengths(train_texts)\n",
    "\n",
    "# Plot sequence length distribution\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(seq_lengths, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "plt.axvline(seq_stats['mean_length'], color='red', linestyle='--', \n",
    "           linewidth=2, label=f\"Mean: {seq_stats['mean_length']:.1f}\")\n",
    "plt.axvline(seq_stats['median_length'], color='green', linestyle='--', \n",
    "           linewidth=2, label=f\"Median: {seq_stats['median_length']:.0f}\")\n",
    "plt.axvline(config.max_len, color='orange', linestyle='--', \n",
    "           linewidth=2, label=f\"MAX_LEN: {config.max_len}\")\n",
    "plt.title('Sequence Length Distribution', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Sequence Length (tokens)', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(seq_lengths, vert=True)\n",
    "plt.axhline(config.max_len, color='orange', linestyle='--', \n",
    "           linewidth=2, label=f\"MAX_LEN: {config.max_len}\")\n",
    "plt.title('Sequence Length Box Plot', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Sequence Length (tokens)', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Justify MAX_LEN choice\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MAX_LEN JUSTIFICATION\")\n",
    "print(\"=\"*80)\n",
    "percentiles = [50, 75, 90, 95, 99]\n",
    "for p in percentiles:\n",
    "    val = np.percentile(seq_lengths, p)\n",
    "    print(f\"{p}th percentile: {val:.0f} tokens\")\n",
    "\n",
    "print(f\"\\nChosen MAX_LEN: {config.max_len}\")\n",
    "print(f\"This covers {(sum(1 for l in seq_lengths if l <= config.max_len) / len(seq_lengths) * 100):.1f}% of sequences\")\n",
    "print(f\"Sequences truncated: {seq_stats['truncated_count']} ({seq_stats['truncation_percent']:.1f}%)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Convert to sequences\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONVERTING TEXTS TO SEQUENCES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X_train = embedding_handler.texts_to_sequences(train_texts)\n",
    "X_val = embedding_handler.texts_to_sequences(val_texts)\n",
    "\n",
    "print(f\"\\nTraining sequences shape: {X_train.shape}\")\n",
    "print(f\"Validation sequences shape: {X_val.shape}\")\n",
    "\n",
    "# Prepare labels\n",
    "y_train = train_df_clean['label'].values\n",
    "y_val = val_df_clean['label'].values\n",
    "\n",
    "# Convert to one-hot encoding\n",
    "y_train_cat = to_categorical(y_train, num_classes=config.num_classes)\n",
    "y_val_cat = to_categorical(y_val, num_classes=config.num_classes)\n",
    "\n",
    "print(f\"\\nTraining labels shape: {y_train_cat.shape}\")\n",
    "print(f\"Validation labels shape: {y_val_cat.shape}\")\n",
    "\n",
    "# Save tokenizer if configured\n",
    "if config.save_tokenizer:\n",
    "    os.makedirs(config.config_dir, exist_ok=True)\n",
    "    tokenizer_path = os.path.join(config.config_dir, 'tokenizer.json')\n",
    "    embedding_handler.save_tokenizer(tokenizer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Section 17: Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#  MODEL BUILDING AND TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or train embeddings based on configuration\n",
    "print(\"=\"*80)\n",
    "print(f\"LOADING {config.embedding_type.upper()} EMBEDDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if config.embedding_type == 'glove':\n",
    "    # Load GloVe embeddings\n",
    "    embedding_handler.load_glove_embeddings(config.glove_path)\n",
    "elif config.embedding_type == 'word2vec':\n",
    "    # Train Word2Vec on our corpus\n",
    "    embedding_handler.train_word2vec(train_texts, min_count=1)\n",
    "\n",
    "# Create embedding matrix\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CREATING EMBEDDING MATRIX\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "embedding_matrix = embedding_handler.create_embedding_matrix()\n",
    "\n",
    "# Display embedding coverage statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EMBEDDING COVERAGE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Vocabulary size: {embedding_handler.stats['vocab_size']}\")\n",
    "print(f\"Words with embeddings: {embedding_handler.stats['coverage_count']} ({embedding_handler.stats['coverage_percent']:.2f}%)\")\n",
    "print(f\"OOV words: {embedding_handler.stats['oov_count']} ({embedding_handler.stats['oov_percent']:.2f}%)\")\n",
    "print(f\"\\nFirst 20 OOV words:\")\n",
    "for i, word in enumerate(embedding_handler.stats['oov_words'][:20], 1):\n",
    "    print(f\"  {i:2d}. {word}\")\n",
    "\n",
    "# Calculate OOV rate in actual sequences\n",
    "oov_rate = embedding_handler.get_oov_rate(X_train)\n",
    "print(f\"\\nOOV tokens in training sequences: {oov_rate:.2f}%\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Visualize embedding coverage\n",
    "vocab_size = embedding_handler.stats['vocab_size']\n",
    "coverage_count = embedding_handler.stats['coverage_count']\n",
    "oov_count = embedding_handler.stats['oov_count']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "sizes = [coverage_count, oov_count]\n",
    "labels = [f'Covered\\n{coverage_count} words\\n({embedding_handler.stats[\"coverage_percent\"]:.1f}%)',\n",
    "          f'OOV\\n{oov_count} words\\n({embedding_handler.stats[\"oov_percent\"]:.1f}%)']\n",
    "explode = (0.05, 0.05)\n",
    "\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90, textprops={'fontsize': 12, 'weight': 'bold'})\n",
    "plt.title(f'{config.embedding_type.upper()} Embedding Coverage', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "# Save embedding matrix if configured\n",
    "if config.save_embedding_matrix:\n",
    "    os.makedirs(config.save_dir, exist_ok=True)\n",
    "    embedding_path = os.path.join(config.save_dir, 'embedding_matrix.npy')\n",
    "    embedding_handler.save_embedding_matrix(embedding_path)\n",
    "\n",
    "print(f\"\\n Embedding matrix ready: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Section 18: Train Model with Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model builder\n",
    "model_builder = AdvancedModelBuilder(config)\n",
    "\n",
    "# Build model\n",
    "print(\"=\"*80)\n",
    "print(\"BUILDING MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model = model_builder.build_model(embedding_matrix)\n",
    "\n",
    "# Display model architecture\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL ARCHITECTURE\")\n",
    "print(\"=\"*80)\n",
    "model.summary()\n",
    "\n",
    "# Visualize model architecture (if possible)\n",
    "try:\n",
    "    from tensorflow.keras.utils import plot_model\n",
    "    plot_path = os.path.join(config.save_dir, 'model_architecture.png')\n",
    "    os.makedirs(config.save_dir, exist_ok=True)\n",
    "    plot_model(model, to_file=plot_path, show_shapes=True, show_layer_names=True)\n",
    "    print(f\"\\n Model architecture diagram saved to {plot_path}\")\n",
    "except:\n",
    "    print(\"\\n  Could not generate model diagram (requires graphviz)\")\n",
    "\n",
    "# Compute class weights\n",
    "if config.use_class_weights:\n",
    "    class_weights = preprocessor.compute_class_weights(y_train)\n",
    "else:\n",
    "    class_weights = None\n",
    "    logger.info(\"Class weights disabled\")\n",
    "\n",
    "print(\"\\n Model ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Section 19: Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare directories\n",
    "os.makedirs(config.save_dir, exist_ok=True)\n",
    "os.makedirs(config.log_dir, exist_ok=True)\n",
    "os.makedirs(config.result_dir, exist_ok=True)\n",
    "\n",
    "# Setup callbacks\n",
    "callbacks = []\n",
    "\n",
    "# Experiment tracker\n",
    "experiment_tracker = ExperimentTracker(config)\n",
    "callbacks.append(experiment_tracker)\n",
    "\n",
    "# Model checkpoint\n",
    "checkpoint_path = os.path.join(config.save_dir, f'{config.experiment_name}_best_model.h5')\n",
    "checkpoint = ModelCheckpoint(\n",
    "    checkpoint_path,\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "callbacks.append(checkpoint)\n",
    "\n",
    "# Early stopping\n",
    "if config.early_stopping:\n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=config.patience,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks.append(early_stop)\n",
    "\n",
    "# Reduce LR on plateau\n",
    "if config.reduce_lr:\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=config.lr_factor,\n",
    "        patience=config.lr_patience,\n",
    "        min_lr=config.min_lr,\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks.append(reduce_lr)\n",
    "\n",
    "# TensorBoard\n",
    "tensorboard_dir = os.path.join(config.log_dir, config.experiment_name)\n",
    "tensorboard = TensorBoard(log_dir=tensorboard_dir, histogram_freq=1)\n",
    "callbacks.append(tensorboard)\n",
    "\n",
    "# CSV Logger\n",
    "csv_path = os.path.join(config.log_dir, f'{config.experiment_name}_training.csv')\n",
    "csv_logger = CSVLogger(csv_path)\n",
    "callbacks.append(csv_logger)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Experiment: {config.experiment_name}\")\n",
    "print(f\"Model: {config.model_type.upper()}\")\n",
    "print(f\"Epochs: {config.epochs}\")\n",
    "print(f\"Batch Size: {config.batch_size}\")\n",
    "print(f\"Learning Rate: {config.learning_rate}\")\n",
    "print(f\"Class Weights: {'Enabled' if config.use_class_weights else 'Disabled'}\")\n",
    "print(f\"Early Stopping: {'Enabled (patience={})'.format(config.patience) if config.early_stopping else 'Disabled'}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Train model\n",
    "start_time = time.time()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train_cat,\n",
    "    validation_data=(X_val, y_val_cat),\n",
    "    epochs=config.epochs,\n",
    "    batch_size=config.batch_size,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks,\n",
    "    verbose=config.verbose\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETED!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total training time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
    "print(f\"Best val accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
    "print(f\"Best val loss: {min(history.history['val_loss']):.4f}\")\n",
    "print(f\"Model saved to: {checkpoint_path}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Section 20: Make Predictions and Calculate Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#  MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "visualizer.plot_training_history(\n",
    "    history,\n",
    "    save_path=os.path.join(config.result_dir, f'{config.experiment_name}_training_history.png')\n",
    ")\n",
    "\n",
    "# Display training statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Epochs trained: {len(history.history['loss'])}\")\n",
    "print(f\"\\nFinal metrics:\")\n",
    "print(f\"  Train accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"  Train loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"  Val accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "print(f\"  Val loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "print(f\"\\nBest metrics:\")\n",
    "print(f\"  Best val accuracy: {max(history.history['val_accuracy']):.4f} (epoch {np.argmax(history.history['val_accuracy'])+1})\")\n",
    "print(f\"  Best val loss: {min(history.history['val_loss']):.4f} (epoch {np.argmin(history.history['val_loss'])+1})\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Section 21: Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "print(\"=\"*80)\n",
    "print(\"MAKING PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "y_train_pred = model.predict(X_train, verbose=0)\n",
    "y_val_pred = model.predict(X_val, verbose=0)\n",
    "\n",
    "# Convert to class labels\n",
    "y_train_pred_labels = y_train_pred.argmax(axis=1)\n",
    "y_val_pred_labels = y_val_pred.argmax(axis=1)\n",
    "\n",
    "# Calculate comprehensive metrics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VALIDATION SET METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Overall accuracy\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred_labels)\n",
    "print(f\"\\nOverall Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Per-class metrics\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    y_val, y_val_pred_labels, labels=list(range(config.num_classes))\n",
    ")\n",
    "\n",
    "# Macro averages\n",
    "macro_precision = np.mean(precision)\n",
    "macro_recall = np.mean(recall)\n",
    "macro_f1 = np.mean(f1)\n",
    "\n",
    "print(f\"\\nMacro-averaged metrics:\")\n",
    "print(f\"  Precision: {macro_precision:.4f}\")\n",
    "print(f\"  Recall: {macro_recall:.4f}\")\n",
    "print(f\"  F1-Score: {macro_f1:.4f}\")\n",
    "\n",
    "# Per-class breakdown\n",
    "print(f\"\\nPer-class metrics:\")\n",
    "print(f\"{'Emotion':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<12}\")\n",
    "print(\"-\" * 60)\n",
    "for i, emotion in emotion_map.items():\n",
    "    print(f\"{emotion.capitalize():<12} {precision[i]:<12.4f} {recall[i]:<12.4f} {f1[i]:<12.4f} {support[i]:<12}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Store metrics for later use\n",
    "metrics = {\n",
    "    'val_accuracy': val_accuracy,\n",
    "    'val_loss': history.history['val_loss'][-1],\n",
    "    'macro_precision': macro_precision,\n",
    "    'macro_recall': macro_recall,\n",
    "    'macro_f1': macro_f1,\n",
    "    'per_class_precision': precision.tolist(),\n",
    "    'per_class_recall': recall.tolist(),\n",
    "    'per_class_f1': f1.tolist(),\n",
    "    'training_time': training_time\n",
    "}\n",
    "\n",
    "# Save metrics to JSON\n",
    "metrics_path = os.path.join(config.result_dir, f'{config.experiment_name}_metrics.json')\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "print(f\"\\n Metrics saved to {metrics_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Section 22: Classification Report Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot raw confusion matrix\n",
    "visualizer.plot_confusion_matrix(\n",
    "    y_val, y_val_pred_labels, \n",
    "    normalize=False,\n",
    "    save_path=os.path.join(config.result_dir, f'{config.experiment_name}_confusion_matrix.png')\n",
    ")\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "visualizer.plot_confusion_matrix(\n",
    "    y_val, y_val_pred_labels, \n",
    "    normalize=True,\n",
    "    save_path=os.path.join(config.result_dir, f'{config.experiment_name}_confusion_matrix_normalized.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Section 23: Per-Class F1 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot classification report heatmap\n",
    "metrics_df = visualizer.plot_classification_report(\n",
    "    y_val, y_val_pred_labels,\n",
    "    save_path=os.path.join(config.result_dir, f'{config.experiment_name}_classification_report.png')\n",
    ")\n",
    "\n",
    "print(\"\\nClassification Report DataFrame:\")\n",
    "print(metrics_df)\n",
    "\n",
    "# Save to CSV\n",
    "metrics_df.to_csv(os.path.join(config.result_dir, f'{config.experiment_name}_classification_report.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Section 24: Model Comparison (Run multiple experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Section 25: View Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model comparer\n",
    "comparer = ModelComparer(emotion_map)\n",
    "\n",
    "# Add current experiment\n",
    "comparer.add_experiment(\n",
    "    name=config.experiment_name,\n",
    "    config=config,\n",
    "    history=history,\n",
    "    metrics=metrics,\n",
    "    predictions=y_val_pred_labels,\n",
    "    y_true=y_val\n",
    ")\n",
    "\n",
    "print(\" Current experiment added to comparer\")\n",
    "print(\"\\nTo add more experiments:\")\n",
    "print(\"1. Modify config in Section 2 (e.g., config.model_type = 'gru')\")\n",
    "print(\"2. Re-run sections 15-23\")\n",
    "print(\"3. Run: comparer.add_experiment(name='experiment_2', config=config, history=history, metrics=metrics)\")\n",
    "print(\"4. View comparison: comparer.create_comparison_table()\")\n",
    "\n",
    "# Example: Uncomment and modify to add more experiments\n",
    "# config.model_type = 'gru'\n",
    "# config.experiment_name = 'gru_128'\n",
    "# # Re-run training...\n",
    "# comparer.add_experiment(name='gru_128', config=config, history=history, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Section 26: Prediction Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#  PREDICTIONS AND INTERACTIVE TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comparison table\n",
    "comparison_df = comparer.create_comparison_table()\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL COMPARISON TABLE\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Plot comparison\n",
    "if len(comparer.experiments) > 1:\n",
    "    comparer.plot_comparison(metric='val_accuracy')\n",
    "    comparer.plot_comparison(metric='macro_f1')\n",
    "    \n",
    "    # Save comparison\n",
    "    comparison_path = os.path.join(config.result_dir, 'model_comparison.csv')\n",
    "    comparer.save_comparison(comparison_path)\n",
    "else:\n",
    "    print(\"\\n Add more experiments to enable comparison visualizations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Section 27: Test Predictions with Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(text: str, show_probabilities: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Predict emotion for a given text.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "        show_probabilities: Whether to show probabilities for all classes\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with prediction results\n",
    "    \"\"\"\n",
    "    # Preprocess text\n",
    "    cleaned_text = preprocessor.clean_text(text)\n",
    "    \n",
    "    # Convert to sequence\n",
    "    sequence = tokenizer.texts_to_sequences([cleaned_text])\n",
    "    padded = pad_sequences(sequence, maxlen=config.max_len, padding='post', truncating='post')\n",
    "    \n",
    "    # Predict\n",
    "    predictions = model.predict(padded, verbose=0)[0]\n",
    "    predicted_label = predictions.argmax()\n",
    "    predicted_emotion = emotion_map[predicted_label]\n",
    "    confidence = predictions[predicted_label]\n",
    "    \n",
    "    result = {\n",
    "        'original_text': text,\n",
    "        'cleaned_text': cleaned_text,\n",
    "        'predicted_emotion': predicted_emotion,\n",
    "        'predicted_label': int(predicted_label),\n",
    "        'confidence': float(confidence)\n",
    "    }\n",
    "    \n",
    "    if show_probabilities:\n",
    "        result['all_probabilities'] = {\n",
    "            emotion_map[i]: float(predictions[i]) \n",
    "            for i in range(len(predictions))\n",
    "        }\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def display_prediction(text: str):\n",
    "    \"\"\"Display prediction with formatting.\"\"\"\n",
    "    result = predict_emotion(text, show_probabilities=True)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"EMOTION PREDICTION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Original Text: {result['original_text']}\")\n",
    "    print(f\"Cleaned Text:  {result['cleaned_text']}\")\n",
    "    print(f\"\\n Predicted Emotion: {result['predicted_emotion'].upper()}\")\n",
    "    print(f\"   Confidence: {result['confidence']*100:.2f}%\")\n",
    "    print(f\"\\nAll Probabilities:\")\n",
    "    for emotion, prob in sorted(result['all_probabilities'].items(), key=lambda x: x[1], reverse=True):\n",
    "        bar = '' * int(prob * 50)\n",
    "        print(f\"  {emotion.capitalize():<12} {prob*100:5.2f}% {bar}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "\n",
    "print(\" Prediction functions created!\")\n",
    "print(\"\\nUsage:\")\n",
    "print(\"  display_prediction('I am so happy today!')\")\n",
    "print(\"  result = predict_emotion('I feel terrible', show_probabilities=True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Section 28: Pipeline Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#  FINAL SUMMARY AND NEXT STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample texts for each emotion\n",
    "test_samples = [\n",
    "    \"I am so sad and depressed today\",\n",
    "    \"This is the happiest day of my life!\",\n",
    "    \"I love you so much, you mean everything to me\",\n",
    "    \"I am so angry and furious right now!\",\n",
    "    \"I'm really scared and terrified about this\",\n",
    "    \"Wow! I can't believe this happened, so unexpected!\"\n",
    "]\n",
    "\n",
    "print(\"Testing predictions on sample texts:\\n\")\n",
    "for i, text in enumerate(test_samples, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Example {i}:\")\n",
    "    display_prediction(text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\" PIPELINE EXECUTION COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n FINAL RESULTS SUMMARY\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Experiment Name: {config.experiment_name}\")\n",
    "print(f\"Model Type: {config.model_type.upper()}\")\n",
    "print(f\"Embedding: {config.embedding_type.upper()} {config.embedding_dim}d\")\n",
    "print(f\"RNN Units: {config.rnn_units}\")\n",
    "print(f\"Layers: {config.num_rnn_layers}\")\n",
    "print(f\"Trainable Embeddings: {config.trainable_embeddings}\")\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Training samples: {len(X_train)}\")\n",
    "print(f\"  Validation samples: {len(X_val)}\")\n",
    "print(f\"  Vocabulary size: {embedding_handler.stats['vocab_size']}\")\n",
    "print(f\"  Embedding coverage: {embedding_handler.stats['coverage_percent']:.2f}%\")\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  Validation Accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")\n",
    "print(f\"  Macro F1-Score: {macro_f1:.4f}\")\n",
    "print(f\"  Training Time: {training_time:.2f}s ({training_time/60:.2f} min)\")\n",
    "print(f\"  Epochs Trained: {len(history.history['loss'])}\")\n",
    "print(f\"\\nFiles Saved:\")\n",
    "print(f\"   Model: {checkpoint_path}\")\n",
    "print(f\"   Metrics: {metrics_path}\")\n",
    "print(f\"   Training Log: {csv_path}\")\n",
    "if config.save_tokenizer:\n",
    "    print(f\"   Tokenizer: {os.path.join(config.config_dir, 'tokenizer.json')}\")\n",
    "if config.save_config:\n",
    "    config_path = os.path.join(config.config_dir, f'{config.experiment_name}_config.json')\n",
    "    config.save_to_json(config_path)\n",
    "    print(f\"   Config: {config_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" NEXT STEPS FOR HYPERPARAMETER EXPERIMENTATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "1. COMPARE LSTM VS GRU:\n",
    "   - Set config.model_type = 'gru' (or 'bilstm', 'bigru')\n",
    "   - Re-run from Section 15 onwards\n",
    "   - Use comparer.add_experiment() to track results\n",
    "\n",
    "2. TRY DIFFERENT EMBEDDINGS:\n",
    "   - Set config.embedding_type = 'word2vec'\n",
    "   - Or change config.embedding_dim = 50 (for glove.6B.50d.txt)\n",
    "   - Re-run from Section 16 onwards\n",
    "\n",
    "3. ADJUST ARCHITECTURE:\n",
    "   - config.rnn_units = 64 or 256\n",
    "   - config.num_rnn_layers = 2\n",
    "   - config.use_layer_norm = True\n",
    "   - Re-run from Section 17 onwards\n",
    "\n",
    "4. TUNE REGULARIZATION:\n",
    "   - config.dropout = 0.3 or 0.5\n",
    "   - config.spatial_dropout = 0.3\n",
    "   - Re-run from Section 17 onwards\n",
    "\n",
    "5. EXPERIMENT WITH TRAINABLE EMBEDDINGS:\n",
    "   - config.trainable_embeddings = True\n",
    "   - Re-run from Section 17 onwards\n",
    "\n",
    "6. COMPARE RESULTS:\n",
    "   - View: comparer.create_comparison_table()\n",
    "   - Plot: comparer.plot_comparison('val_accuracy')\n",
    "   - Save: comparer.save_comparison('results/comparison.csv')\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" All done! Happy experimenting! \")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.9.21)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}