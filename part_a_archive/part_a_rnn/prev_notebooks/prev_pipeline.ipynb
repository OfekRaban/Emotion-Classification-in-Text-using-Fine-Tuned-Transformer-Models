{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Unified Complete Emotion Detection Pipeline\n",
    "## All-in-One Professional Deep Learning System\n",
    "\n",
    "This notebook contains **everything** in one place:\n",
    "- All classes and functions\n",
    "- Complete preprocessing\n",
    "- Multiple model architectures (LSTM, GRU, Bidirectional)\n",
    "- Professional training pipeline\n",
    "- Comprehensive visualizations\n",
    "- Experiment tracking\n",
    "\n",
    "**No external imports from src/ needed - completely self-contained!**\n",
    "\n",
    "### Emotion Classes:\n",
    "0. Sadness üò¢\n",
    "1. Joy üòä\n",
    "2. Love ‚ù§Ô∏è\n",
    "3. Anger üò†\n",
    "4. Fear üò®\n",
    "5. Surprise üò≤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Section 1: Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, ModelCheckpoint, ReduceLROnPlateau,\n",
    "    TensorBoard, CSVLogger, Callback\n",
    ")\n",
    "\n",
    "# Embeddings\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Section 2: Configuration Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Complete configuration for emotion detection pipeline.\"\"\"\n",
    "    \n",
    "    # Experiment\n",
    "    experiment_name: str = \"emotion_detection_unified\"\n",
    "    \n",
    "    # Data paths\n",
    "    train_path: str = \"/home/lab/rabanof/projects/Emotion_Detection_DL/data/raw/train.csv\"\n",
    "    val_path: str = \"/home/lab/rabanof/projects/Emotion_Detection_DL/data/raw/validation.csv\"\n",
    "    glove_path: str = \"/home/lab/rabanof/Emotion_Detection_DL/glove/glove.6B.100d.txt\"\n",
    "    \n",
    "    # Data parameters\n",
    "    max_len: int = 60\n",
    "    max_words: int = 20000\n",
    "    text_column: str = \"text\"\n",
    "    label_column: str = \"label\"\n",
    "    \n",
    "    # Embedding\n",
    "    embedding_type: str = \"glove\"  # 'glove' or 'word2vec'\n",
    "    embedding_dim: int = 100\n",
    "    trainable_embeddings: bool = False\n",
    "    oov_token: str = \"<UNK>\"\n",
    "    \n",
    "    # Model architecture\n",
    "    model_type: str = \"lstm\"  # 'lstm', 'gru', or 'bilstm'\n",
    "    rnn_units: int = 128\n",
    "    num_layers: int = 1\n",
    "    dropout: float = 0.2\n",
    "    recurrent_dropout: float = 0.0\n",
    "    spatial_dropout: float = 0.2\n",
    "    dense_units: int = 0\n",
    "    num_classes: int = 6\n",
    "    \n",
    "    # Training\n",
    "    epochs: int = 50\n",
    "    batch_size: int = 32\n",
    "    learning_rate: float = 0.001\n",
    "    use_class_weights: bool = True\n",
    "    \n",
    "    # Callbacks\n",
    "    early_stopping: bool = True\n",
    "    patience: int = 5\n",
    "    reduce_lr: bool = True\n",
    "    lr_factor: float = 0.5\n",
    "    lr_patience: int = 3\n",
    "    min_lr: float = 1e-7\n",
    "    \n",
    "    # Directories\n",
    "    save_dir: str = \"saved_models\"\n",
    "    log_dir: str = \"logs\"\n",
    "    result_dir: str = \"results\"\n",
    "\n",
    "# Create default configuration\n",
    "config = Config()\n",
    "print(\"‚úÖ Configuration created!\")\n",
    "print(f\"Experiment: {config.experiment_name}\")\n",
    "print(f\"Model: {config.model_type.upper()}, Units: {config.rnn_units}\")\n",
    "print(f\"Embedding: {config.embedding_type.upper()}, Dim: {config.embedding_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Section 3: Text Preprocessing Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"Advanced text preprocessing for emotion detection.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Specific contractions (30+ rules)\n",
    "        self.specific_contractions = {\n",
    "            \"didnt\": \"did not\", \"dont\": \"do not\", \"cant\": \"cannot\",\n",
    "            \"wont\": \"will not\", \"wouldnt\": \"would not\", \"shouldnt\": \"should not\",\n",
    "            \"couldnt\": \"could not\", \"im\": \"i am\", \"ive\": \"i have\",\n",
    "            \"id\": \"i would\", \"ill\": \"i will\", \"hadnt\": \"had not\",\n",
    "            \"youve\": \"you have\", \"werent\": \"were not\", \"theyve\": \"they have\",\n",
    "            \"theyll\": \"they will\", \"itll\": \"it will\", \"couldve\": \"could have\",\n",
    "            \"shouldve\": \"should have\", \"wouldve\": \"would have\"\n",
    "        }\n",
    "        \n",
    "        self.general_contractions = {\n",
    "            \"n't\": \" not\", \"'re\": \" are\", \"'s\": \" is\",\n",
    "            \"'d\": \" would\", \"'ll\": \" will\", \"'t\": \" not\",\n",
    "            \"'ve\": \" have\", \"'m\": \" am\"\n",
    "        }\n",
    "        \n",
    "        self.slang_corrections = {\n",
    "            \"idk\": \"i do not know\", \"yknow\": \"you know\",\n",
    "            \"becuz\": \"because\", \"alittle\": \"a little\", \"incase\": \"in case\"\n",
    "        }\n",
    "        \n",
    "        self.typo_corrections = {\n",
    "            \"vunerable\": \"vulnerable\", \"percieve\": \"perceive\",\n",
    "            \"definetly\": \"definitely\", \"writting\": \"writing\"\n",
    "        }\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Apply comprehensive text cleaning.\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Reduce elongation (sooooo ‚Üí soo)\n",
    "        text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "        \n",
    "        # Expand contractions\n",
    "        for key, value in self.specific_contractions.items():\n",
    "            text = re.sub(rf'\\b{re.escape(key)}\\b', value, text)\n",
    "        \n",
    "        for key, value in self.general_contractions.items():\n",
    "            text = text.replace(key, value)\n",
    "        \n",
    "        # Fix slang and typos\n",
    "        for key, value in {**self.slang_corrections, **self.typo_corrections}.items():\n",
    "            text = re.sub(rf'\\b{re.escape(key)}\\b', value, text)\n",
    "        \n",
    "        # Reduce repeated punctuation\n",
    "        text = re.sub(r\"([!?.,])\\1+\", r\"\\1\", text)\n",
    "        text = re.sub(r\"\\.{2,}\", \".\", text)\n",
    "        \n",
    "        # Normalize whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def preprocess_dataframe(self, df: pd.DataFrame, text_column: str = 'text') -> pd.DataFrame:\n",
    "        \"\"\"Preprocess entire dataframe.\"\"\"\n",
    "        df = df.copy()\n",
    "        logger.info(f\"Preprocessing {len(df)} samples...\")\n",
    "        \n",
    "        df[text_column] = df[text_column].apply(self.clean_text)\n",
    "        df['text_len'] = df[text_column].str.split().str.len()\n",
    "        \n",
    "        logger.info(f\"Mean text length: {df['text_len'].mean():.2f} words\")\n",
    "        return df\n",
    "    \n",
    "    def remove_duplicates(self, df: pd.DataFrame, text_column: str = 'text') -> pd.DataFrame:\n",
    "        \"\"\"Remove duplicate texts.\"\"\"\n",
    "        initial_len = len(df)\n",
    "        df = df.drop_duplicates(subset=[text_column], keep='first')\n",
    "        removed = initial_len - len(df)\n",
    "        if removed > 0:\n",
    "            logger.warning(f\"Removed {removed} duplicates\")\n",
    "        return df.reset_index(drop=True)\n",
    "    \n",
    "    def check_data_leakage(self, train_df: pd.DataFrame, val_df: pd.DataFrame,\n",
    "                          text_column: str = 'text') -> Tuple[pd.DataFrame, int]:\n",
    "        \"\"\"Check and remove overlapping texts.\"\"\"\n",
    "        train_texts = set(train_df[text_column])\n",
    "        val_texts = set(val_df[text_column])\n",
    "        overlaps = val_texts.intersection(train_texts)\n",
    "        \n",
    "        if len(overlaps) > 0:\n",
    "            logger.warning(f\"Found {len(overlaps)} overlapping texts\")\n",
    "            val_df_clean = val_df[~val_df[text_column].isin(overlaps)].copy()\n",
    "            return val_df_clean.reset_index(drop=True), len(overlaps)\n",
    "        \n",
    "        return val_df, 0\n",
    "    \n",
    "    def compute_class_weights(self, labels: np.ndarray) -> Dict[int, float]:\n",
    "        \"\"\"Compute class weights for imbalanced data.\"\"\"\n",
    "        classes = np.unique(labels)\n",
    "        weights = compute_class_weight('balanced', classes=classes, y=labels)\n",
    "        class_weights = dict(zip(classes, weights))\n",
    "        logger.info(f\"Class weights: {class_weights}\")\n",
    "        return class_weights\n",
    "\n",
    "print(\"‚úÖ TextPreprocessor class created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî§ Section 4: Embedding Handler Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingHandler:\n",
    "    \"\"\"Handle embeddings (GloVe/Word2Vec) and sequence generation.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_type='glove', embedding_dim=100, \n",
    "                 max_words=20000, max_len=60, oov_token='<UNK>'):\n",
    "        self.embedding_type = embedding_type\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_words = max_words\n",
    "        self.max_len = max_len\n",
    "        self.oov_token = oov_token\n",
    "        \n",
    "        self.tokenizer = None\n",
    "        self.embedding_matrix = None\n",
    "        self.embeddings_index = {}\n",
    "        self.vocab_size = 0\n",
    "    \n",
    "    def load_glove_embeddings(self, glove_path: str) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Load GloVe pre-trained embeddings.\"\"\"\n",
    "        logger.info(f\"Loading GloVe from {glove_path}\")\n",
    "        embeddings_index = {}\n",
    "        \n",
    "        with open(glove_path, encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs\n",
    "        \n",
    "        logger.info(f\"Loaded {len(embeddings_index)} word vectors\")\n",
    "        self.embeddings_index = embeddings_index\n",
    "        return embeddings_index\n",
    "    \n",
    "    def train_word2vec(self, texts: list, vector_size=100, window=5, \n",
    "                      min_count=2, workers=4, epochs=10):\n",
    "        \"\"\"Train Word2Vec on corpus.\"\"\"\n",
    "        logger.info(\"Training Word2Vec...\")\n",
    "        tokenized_texts = [text.split() for text in texts]\n",
    "        \n",
    "        model = Word2Vec(\n",
    "            sentences=tokenized_texts,\n",
    "            vector_size=vector_size,\n",
    "            window=window,\n",
    "            min_count=min_count,\n",
    "            workers=workers,\n",
    "            epochs=epochs\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Word2Vec trained, vocab size: {len(model.wv)}\")\n",
    "        self.embeddings_index = {word: model.wv[word] for word in model.wv.index_to_key}\n",
    "        return model\n",
    "    \n",
    "    def create_tokenizer(self, texts: list):\n",
    "        \"\"\"Create and fit tokenizer.\"\"\"\n",
    "        logger.info(\"Creating tokenizer...\")\n",
    "        tokenizer = Tokenizer(num_words=self.max_words, oov_token=self.oov_token)\n",
    "        tokenizer.fit_on_texts(texts)\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = min(self.max_words, len(tokenizer.word_index) + 1)\n",
    "        logger.info(f\"Vocabulary size: {self.vocab_size}\")\n",
    "        return tokenizer\n",
    "    \n",
    "    def texts_to_sequences(self, texts: list, pad=True) -> np.ndarray:\n",
    "        \"\"\"Convert texts to padded sequences.\"\"\"\n",
    "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "        if pad:\n",
    "            sequences = pad_sequences(sequences, maxlen=self.max_len, \n",
    "                                     padding='post', truncating='post')\n",
    "        return sequences\n",
    "    \n",
    "    def create_embedding_matrix(self):\n",
    "        \"\"\"Create embedding matrix from loaded embeddings.\"\"\"\n",
    "        logger.info(\"Creating embedding matrix...\")\n",
    "        embedding_matrix = np.zeros((self.vocab_size, self.embedding_dim))\n",
    "        words_found = 0\n",
    "        words_not_found = []\n",
    "        \n",
    "        for word, idx in self.tokenizer.word_index.items():\n",
    "            if idx >= self.max_words:\n",
    "                continue\n",
    "            \n",
    "            embedding_vector = self.embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[idx] = embedding_vector\n",
    "                words_found += 1\n",
    "            else:\n",
    "                words_not_found.append(word)\n",
    "                embedding_matrix[idx] = np.random.normal(0, 0.1, self.embedding_dim)\n",
    "        \n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        coverage = (words_found / self.vocab_size) * 100\n",
    "        \n",
    "        logger.info(f\"Coverage: {coverage:.2f}% ({words_found}/{self.vocab_size})\")\n",
    "        logger.info(f\"Sample OOV: {words_not_found[:10]}\")\n",
    "        \n",
    "        return embedding_matrix, {\n",
    "            'coverage_percent': coverage,\n",
    "            'words_found': words_found,\n",
    "            'words_not_found': len(words_not_found)\n",
    "        }\n",
    "    \n",
    "    def get_oov_rate(self, sequences: np.ndarray) -> float:\n",
    "        \"\"\"Calculate OOV rate in sequences.\"\"\"\n",
    "        oov_index = self.tokenizer.word_index.get(self.oov_token, 1)\n",
    "        total_tokens = np.count_nonzero(sequences)\n",
    "        oov_tokens = np.sum(sequences == oov_index)\n",
    "        oov_rate = (oov_tokens / total_tokens * 100) if total_tokens > 0 else 0\n",
    "        return oov_rate\n",
    "\n",
    "print(\"‚úÖ EmbeddingHandler class created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Section 5: Model Builder Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBuilder:\n",
    "    \"\"\"Build LSTM/GRU models for emotion detection.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, embedding_matrix=None, \n",
    "                 max_len=60, num_classes=6):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.max_len = max_len\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def build_lstm(self, units=128, num_layers=1, dropout=0.2, \n",
    "                   spatial_dropout=0.2, bidirectional=False, \n",
    "                   trainable_embeddings=False):\n",
    "        \"\"\"Build LSTM model.\"\"\"\n",
    "        model = models.Sequential(name='LSTM_Model')\n",
    "        \n",
    "        # Embedding\n",
    "        if self.embedding_matrix is not None:\n",
    "            model.add(layers.Embedding(\n",
    "                input_dim=self.vocab_size,\n",
    "                output_dim=self.embedding_dim,\n",
    "                weights=[self.embedding_matrix],\n",
    "                input_length=self.max_len,\n",
    "                trainable=trainable_embeddings,\n",
    "                name='embedding'\n",
    "            ))\n",
    "        else:\n",
    "            model.add(layers.Embedding(\n",
    "                input_dim=self.vocab_size,\n",
    "                output_dim=self.embedding_dim,\n",
    "                input_length=self.max_len,\n",
    "                name='embedding'\n",
    "            ))\n",
    "        \n",
    "        # Spatial Dropout\n",
    "        if spatial_dropout > 0:\n",
    "            model.add(layers.SpatialDropout1D(spatial_dropout))\n",
    "        \n",
    "        # LSTM layers\n",
    "        for i in range(num_layers):\n",
    "            return_sequences = (i < num_layers - 1)\n",
    "            lstm_layer = layers.LSTM(\n",
    "                units=units,\n",
    "                dropout=dropout,\n",
    "                recurrent_dropout=0.0,\n",
    "                return_sequences=return_sequences,\n",
    "                name=f'lstm_{i+1}'\n",
    "            )\n",
    "            \n",
    "            if bidirectional:\n",
    "                lstm_layer = layers.Bidirectional(lstm_layer, name=f'bi_lstm_{i+1}')\n",
    "            \n",
    "            model.add(lstm_layer)\n",
    "        \n",
    "        # Output\n",
    "        model.add(layers.Dense(self.num_classes, activation='softmax', name='output'))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def build_gru(self, units=128, num_layers=1, dropout=0.2, \n",
    "                  spatial_dropout=0.2, bidirectional=False, \n",
    "                  trainable_embeddings=False):\n",
    "        \"\"\"Build GRU model.\"\"\"\n",
    "        model = models.Sequential(name='GRU_Model')\n",
    "        \n",
    "        # Embedding\n",
    "        if self.embedding_matrix is not None:\n",
    "            model.add(layers.Embedding(\n",
    "                input_dim=self.vocab_size,\n",
    "                output_dim=self.embedding_dim,\n",
    "                weights=[self.embedding_matrix],\n",
    "                input_length=self.max_len,\n",
    "                trainable=trainable_embeddings,\n",
    "                name='embedding'\n",
    "            ))\n",
    "        else:\n",
    "            model.add(layers.Embedding(\n",
    "                input_dim=self.vocab_size,\n",
    "                output_dim=self.embedding_dim,\n",
    "                input_length=self.max_len,\n",
    "                name='embedding'\n",
    "            ))\n",
    "        \n",
    "        # Spatial Dropout\n",
    "        if spatial_dropout > 0:\n",
    "            model.add(layers.SpatialDropout1D(spatial_dropout))\n",
    "        \n",
    "        # GRU layers\n",
    "        for i in range(num_layers):\n",
    "            return_sequences = (i < num_layers - 1)\n",
    "            gru_layer = layers.GRU(\n",
    "                units=units,\n",
    "                dropout=dropout,\n",
    "                recurrent_dropout=0.0,\n",
    "                return_sequences=return_sequences,\n",
    "                name=f'gru_{i+1}'\n",
    "            )\n",
    "            \n",
    "            if bidirectional:\n",
    "                gru_layer = layers.Bidirectional(gru_layer, name=f'bi_gru_{i+1}')\n",
    "            \n",
    "            model.add(gru_layer)\n",
    "        \n",
    "        # Output\n",
    "        model.add(layers.Dense(self.num_classes, activation='softmax', name='output'))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def compile_model(self, model, learning_rate=0.001):\n",
    "        \"\"\"Compile model.\"\"\"\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model\n",
    "\n",
    "print(\"‚úÖ ModelBuilder class created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Section 6: Visualization Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultsVisualizer:\n",
    "    \"\"\"Comprehensive visualization for results.\"\"\"\n",
    "    \n",
    "    def __init__(self, emotion_labels=None):\n",
    "        if emotion_labels is None:\n",
    "            self.emotion_labels = ['Sadness', 'Joy', 'Love', 'Anger', 'Fear', 'Surprise']\n",
    "        else:\n",
    "            self.emotion_labels = emotion_labels\n",
    "    \n",
    "    def plot_training_history(self, history):\n",
    "        \"\"\"Plot training history.\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Accuracy\n",
    "        axes[0].plot(history.history['accuracy'], label='Train', marker='o')\n",
    "        axes[0].plot(history.history['val_accuracy'], label='Validation', marker='s')\n",
    "        axes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Accuracy')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Loss\n",
    "        axes[1].plot(history.history['loss'], label='Train', marker='o')\n",
    "        axes[1].plot(history.history['val_loss'], label='Validation', marker='s')\n",
    "        axes[1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Loss')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_confusion_matrix(self, y_true, y_pred, normalize=False):\n",
    "        \"\"\"Plot confusion matrix.\"\"\"\n",
    "        if y_true.ndim > 1:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "        if y_pred.ndim > 1:\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "        \n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        if normalize:\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            fmt = '.2f'\n",
    "            title = 'Normalized Confusion Matrix'\n",
    "        else:\n",
    "            fmt = 'd'\n",
    "            title = 'Confusion Matrix'\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt=fmt, cmap='Blues',\n",
    "                   xticklabels=self.emotion_labels,\n",
    "                   yticklabels=self.emotion_labels)\n",
    "        plt.title(title, fontsize=14, fontweight='bold')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_classification_report(self, y_true, y_pred):\n",
    "        \"\"\"Plot classification report.\"\"\"\n",
    "        if y_true.ndim > 1:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "        if y_pred.ndim > 1:\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "        \n",
    "        report = classification_report(y_true, y_pred, \n",
    "                                      target_names=self.emotion_labels,\n",
    "                                      output_dict=True)\n",
    "        \n",
    "        df_report = pd.DataFrame(report).transpose()\n",
    "        \n",
    "        # Plot\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        metrics = ['precision', 'recall', 'f1-score']\n",
    "        df_plot = df_report.loc[self.emotion_labels, metrics]\n",
    "        \n",
    "        df_plot.plot(kind='bar', ax=ax, width=0.8)\n",
    "        ax.set_title('Classification Metrics by Emotion', fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('Emotion')\n",
    "        ax.set_ylabel('Score')\n",
    "        ax.set_ylim([0, 1.0])\n",
    "        ax.legend(title='Metric')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print report\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"Classification Report:\")\n",
    "        print(\"=\"*70)\n",
    "        print(classification_report(y_true, y_pred, target_names=self.emotion_labels))\n",
    "    \n",
    "    def plot_per_class_accuracy(self, y_true, y_pred):\n",
    "        \"\"\"Plot per-class accuracy.\"\"\"\n",
    "        if y_true.ndim > 1:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "        if y_pred.ndim > 1:\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "        \n",
    "        accuracies = []\n",
    "        for i in range(len(self.emotion_labels)):\n",
    "            mask = y_true == i\n",
    "            if mask.sum() > 0:\n",
    "                acc = (y_pred[mask] == i).sum() / mask.sum()\n",
    "                accuracies.append(acc)\n",
    "            else:\n",
    "                accuracies.append(0)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(self.emotion_labels, accuracies, edgecolor='black')\n",
    "        \n",
    "        # Color by performance\n",
    "        for i, bar in enumerate(bars):\n",
    "            if accuracies[i] >= 0.8:\n",
    "                bar.set_color('green')\n",
    "            elif accuracies[i] >= 0.6:\n",
    "                bar.set_color('orange')\n",
    "            else:\n",
    "                bar.set_color('red')\n",
    "        \n",
    "        plt.title('Per-Class Accuracy', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Emotion')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.ylim([0, 1.0])\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, acc in enumerate(accuracies):\n",
    "            plt.text(i, acc + 0.02, f'{acc:.2%}', ha='center', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"‚úÖ ResultsVisualizer class created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Section 7: Experiment Tracker Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentTracker(Callback):\n",
    "    \"\"\"Track experiment metrics and save results.\"\"\"\n",
    "    \n",
    "    def __init__(self, experiment_name, results_dir='results'):\n",
    "        super().__init__()\n",
    "        self.experiment_name = experiment_name\n",
    "        self.results_dir = results_dir\n",
    "        self.start_time = None\n",
    "        self.metrics_history = {'train': {}, 'val': {}}\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.start_time = time.time()\n",
    "        logger.info(f\"Training started: {self.experiment_name}\")\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        for key, value in logs.items():\n",
    "            if key.startswith('val_'):\n",
    "                metric_name = key[4:]\n",
    "                if metric_name not in self.metrics_history['val']:\n",
    "                    self.metrics_history['val'][metric_name] = []\n",
    "                self.metrics_history['val'][metric_name].append(float(value))\n",
    "            else:\n",
    "                if key not in self.metrics_history['train']:\n",
    "                    self.metrics_history['train'][key] = []\n",
    "                self.metrics_history['train'][key].append(float(value))\n",
    "    \n",
    "    def on_train_end(self, logs=None):\n",
    "        training_time = time.time() - self.start_time\n",
    "        \n",
    "        results = {\n",
    "            'experiment_name': self.experiment_name,\n",
    "            'training_time_seconds': training_time,\n",
    "            'total_epochs': len(self.metrics_history['train'].get('loss', [])),\n",
    "            'metrics_history': self.metrics_history,\n",
    "            'final_metrics': {\n",
    "                'train': {k: v[-1] for k, v in self.metrics_history['train'].items() if v},\n",
    "                'val': {k: v[-1] for k, v in self.metrics_history['val'].items() if v}\n",
    "            },\n",
    "            'best_metrics': {\n",
    "                'val_accuracy': max(self.metrics_history['val'].get('accuracy', [0])),\n",
    "                'val_loss': min(self.metrics_history['val'].get('loss', [float('inf')]))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save results\n",
    "        results_file = os.path.join(self.results_dir, f'{self.experiment_name}_results.json')\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"Training completed in {training_time:.2f}s\")\n",
    "        logger.info(f\"Best val_accuracy: {results['best_metrics']['val_accuracy']:.4f}\")\n",
    "\n",
    "print(\"‚úÖ ExperimentTracker class created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Section 8: Main Pipeline - Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion mapping\n",
    "EMOTION_MAP = {0: 'Sadness', 1: 'Joy', 2: 'Love', 3: 'Anger', 4: 'Fear', 5: 'Surprise'}\n",
    "EMOTION_LABELS = list(EMOTION_MAP.values())\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 1: LOADING DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv(config.train_path)\n",
    "val_df = pd.read_csv(config.val_path)\n",
    "\n",
    "print(f\"\\nLoaded {len(train_df)} training samples\")\n",
    "print(f\"Loaded {len(val_df)} validation samples\")\n",
    "\n",
    "# Display samples\n",
    "print(\"\\nSample data:\")\n",
    "display(train_df.head())\n",
    "\n",
    "# Label distribution\n",
    "print(\"\\nLabel Distribution:\")\n",
    "print(train_df['label'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Section 9: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STEP 2: EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Visualize label distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Training set\n",
    "train_counts = train_df['label'].map(EMOTION_MAP).value_counts()\n",
    "axes[0].bar(train_counts.index, train_counts.values, color='skyblue', edgecolor='black')\n",
    "axes[0].set_title('Training Set - Emotion Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Emotion')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Validation set\n",
    "val_counts = val_df['label'].map(EMOTION_MAP).value_counts()\n",
    "axes[1].bar(val_counts.index, val_counts.values, color='lightcoral', edgecolor='black')\n",
    "axes[1].set_title('Validation Set - Emotion Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Emotion')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Text length analysis\n",
    "train_df['text_len'] = train_df['text'].str.split().str.len()\n",
    "val_df['text_len'] = val_df['text'].str.split().str.len()\n",
    "\n",
    "print(\"\\nText Length Statistics:\")\n",
    "print(train_df['text_len'].describe())\n",
    "\n",
    "# Plot text length distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.hist(train_df['text_len'], bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(train_df['text_len'].mean(), color='red', linestyle='--', \n",
    "           label=f'Mean: {train_df[\"text_len\"].mean():.1f}')\n",
    "plt.axvline(train_df['text_len'].median(), color='green', linestyle='--', \n",
    "           label=f'Median: {train_df[\"text_len\"].median():.0f}')\n",
    "plt.title('Text Length Distribution', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Section 10: Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STEP 3: TEXT PREPROCESSING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create preprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Preprocess dataframes\n",
    "train_df = preprocessor.preprocess_dataframe(train_df)\n",
    "val_df = preprocessor.preprocess_dataframe(val_df)\n",
    "\n",
    "# Remove duplicates\n",
    "train_df = preprocessor.remove_duplicates(train_df)\n",
    "\n",
    "# Check data leakage\n",
    "val_df, overlaps = preprocessor.check_data_leakage(train_df, val_df)\n",
    "\n",
    "print(f\"\\nAfter preprocessing:\")\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")\n",
    "print(f\"Overlaps removed: {overlaps}\")\n",
    "\n",
    "# Show examples\n",
    "print(\"\\nSample preprocessed texts:\")\n",
    "for i, row in train_df.head(5).iterrows():\n",
    "    print(f\"[{EMOTION_MAP[row['label']]}] {row['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî§ Section 11: Embedding Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STEP 4: CREATING EMBEDDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create embedding handler\n",
    "embedding_handler = EmbeddingHandler(\n",
    "    embedding_type=config.embedding_type,\n",
    "    embedding_dim=config.embedding_dim,\n",
    "    max_words=config.max_words,\n",
    "    max_len=config.max_len,\n",
    "    oov_token=config.oov_token\n",
    ")\n",
    "\n",
    "# Create tokenizer\n",
    "embedding_handler.create_tokenizer(train_df['text'].tolist())\n",
    "\n",
    "# Load embeddings\n",
    "if config.embedding_type == 'glove':\n",
    "    embedding_handler.load_glove_embeddings(config.glove_path)\n",
    "elif config.embedding_type == 'word2vec':\n",
    "    embedding_handler.train_word2vec(train_df['text'].tolist(), \n",
    "                                     vector_size=config.embedding_dim)\n",
    "\n",
    "# Create embedding matrix\n",
    "embedding_matrix, stats = embedding_handler.create_embedding_matrix()\n",
    "\n",
    "# Convert texts to sequences\n",
    "X_train = embedding_handler.texts_to_sequences(train_df['text'].tolist())\n",
    "X_val = embedding_handler.texts_to_sequences(val_df['text'].tolist())\n",
    "\n",
    "# Prepare labels\n",
    "y_train = to_categorical(train_df['label'].values, num_classes=config.num_classes)\n",
    "y_val = to_categorical(val_df['label'].values, num_classes=config.num_classes)\n",
    "\n",
    "print(f\"\\nData shapes:\")\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"X_val: {X_val.shape}\")\n",
    "print(f\"y_val: {y_val.shape}\")\n",
    "print(f\"Embedding matrix: {embedding_matrix.shape}\")\n",
    "\n",
    "# Check OOV rates\n",
    "train_oov = embedding_handler.get_oov_rate(X_train)\n",
    "val_oov = embedding_handler.get_oov_rate(X_val)\n",
    "print(f\"\\nOOV Rates:\")\n",
    "print(f\"Training: {train_oov:.2f}%\")\n",
    "print(f\"Validation: {val_oov:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Section 12: Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STEP 5: COMPUTING CLASS WEIGHTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if config.use_class_weights:\n",
    "    class_weights = preprocessor.compute_class_weights(train_df['label'].values)\n",
    "    print(\"\\nClass Weights:\")\n",
    "    for label, weight in class_weights.items():\n",
    "        print(f\"{EMOTION_MAP[label]:12s}: {weight:.3f}\")\n",
    "else:\n",
    "    class_weights = None\n",
    "    print(\"\\nClass weights disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Section 13: Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STEP 6: BUILDING MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create model builder\n",
    "builder = ModelBuilder(\n",
    "    vocab_size=embedding_handler.vocab_size,\n",
    "    embedding_dim=config.embedding_dim,\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    max_len=config.max_len,\n",
    "    num_classes=config.num_classes\n",
    ")\n",
    "\n",
    "# Build model based on config\n",
    "if config.model_type == 'lstm':\n",
    "    model = builder.build_lstm(\n",
    "        units=config.rnn_units,\n",
    "        num_layers=config.num_layers,\n",
    "        dropout=config.dropout,\n",
    "        spatial_dropout=config.spatial_dropout,\n",
    "        bidirectional=False,\n",
    "        trainable_embeddings=config.trainable_embeddings\n",
    "    )\n",
    "elif config.model_type == 'gru':\n",
    "    model = builder.build_gru(\n",
    "        units=config.rnn_units,\n",
    "        num_layers=config.num_layers,\n",
    "        dropout=config.dropout,\n",
    "        spatial_dropout=config.spatial_dropout,\n",
    "        bidirectional=False,\n",
    "        trainable_embeddings=config.trainable_embeddings\n",
    "    )\n",
    "elif config.model_type == 'bilstm':\n",
    "    model = builder.build_lstm(\n",
    "        units=config.rnn_units,\n",
    "        num_layers=config.num_layers,\n",
    "        dropout=config.dropout,\n",
    "        spatial_dropout=config.spatial_dropout,\n",
    "        bidirectional=True,\n",
    "        trainable_embeddings=config.trainable_embeddings\n",
    "    )\n",
    "\n",
    "# Compile model\n",
    "model = builder.compile_model(model, learning_rate=config.learning_rate)\n",
    "\n",
    "# Display architecture\n",
    "print(f\"\\nModel: {config.model_type.upper()}\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÇ Section 14: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STEP 7: TRAINING MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(config.save_dir, exist_ok=True)\n",
    "os.makedirs(config.log_dir, exist_ok=True)\n",
    "os.makedirs(config.result_dir, exist_ok=True)\n",
    "\n",
    "# Create callbacks\n",
    "callbacks = []\n",
    "\n",
    "# Model checkpoint\n",
    "checkpoint_path = os.path.join(config.save_dir, f'{config.experiment_name}_best.keras')\n",
    "callbacks.append(ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    "))\n",
    "\n",
    "# Early stopping\n",
    "if config.early_stopping:\n",
    "    callbacks.append(EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=config.patience,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ))\n",
    "\n",
    "# Reduce learning rate\n",
    "if config.reduce_lr:\n",
    "    callbacks.append(ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=config.lr_factor,\n",
    "        patience=config.lr_patience,\n",
    "        min_lr=config.min_lr,\n",
    "        verbose=1\n",
    "    ))\n",
    "\n",
    "# CSV logger\n",
    "csv_path = os.path.join(config.log_dir, f'{config.experiment_name}_training.csv')\n",
    "callbacks.append(CSVLogger(csv_path))\n",
    "\n",
    "# Experiment tracker\n",
    "callbacks.append(ExperimentTracker(config.experiment_name, config.result_dir))\n",
    "\n",
    "# Train model\n",
    "print(f\"\\nTraining {config.model_type.upper()} for {config.epochs} epochs...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=config.epochs,\n",
    "    batch_size=config.batch_size,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Section 15: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STEP 8: MODEL EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "\n",
    "print(f\"\\nValidation Results:\")\n",
    "print(f\"Loss: {val_loss:.4f}\")\n",
    "print(f\"Accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Get predictions\n",
    "y_pred = model.predict(X_val, verbose=0)\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Section 16: Visualization - Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"VISUALIZATION 1: TRAINING HISTORY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "visualizer = ResultsVisualizer(EMOTION_LABELS)\n",
    "visualizer.plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Section 17: Visualization - Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"VISUALIZATION 2: CONFUSION MATRIX\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Raw confusion matrix\n",
    "visualizer.plot_confusion_matrix(y_val, y_pred, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized confusion matrix\n",
    "visualizer.plot_confusion_matrix(y_val, y_pred, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Section 18: Visualization - Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"VISUALIZATION 3: CLASSIFICATION REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "visualizer.plot_classification_report(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Section 19: Visualization - Per-Class Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"VISUALIZATION 4: PER-CLASS ACCURACY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "visualizer.plot_per_class_accuracy(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Section 20: Prediction Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PREDICTION EXAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Sample predictions\n",
    "n_samples = 10\n",
    "sample_indices = np.random.choice(len(val_df), n_samples, replace=False)\n",
    "\n",
    "print(\"\\nSample Predictions:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    text = val_df.iloc[idx]['text']\n",
    "    true_label = np.argmax(y_val[idx])\n",
    "    pred_label = np.argmax(y_pred[idx])\n",
    "    confidence = y_pred[idx][pred_label] * 100\n",
    "    \n",
    "    correct = \"‚úÖ\" if true_label == pred_label else \"‚ùå\"\n",
    "    \n",
    "    print(f\"\\n{correct} Text: {text}\")\n",
    "    print(f\"   True: {EMOTION_MAP[true_label]:12s} | Predicted: {EMOTION_MAP[pred_label]:12s} (confidence: {confidence:.1f}%)\")\n",
    "    \n",
    "    # Top 3 predictions\n",
    "    top3_idx = np.argsort(y_pred[idx])[-3:][::-1]\n",
    "    print(f\"   Top 3: \", end=\"\")\n",
    "    for tidx in top3_idx:\n",
    "        print(f\"{EMOTION_MAP[tidx]} ({y_pred[idx][tidx]*100:.1f}%)  \", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÆ Section 21: Interactive Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(text: str, show_probabilities: bool = True):\n",
    "    \"\"\"\n",
    "    Predict emotion for a given text.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "        show_probabilities: Whether to show all class probabilities\n",
    "    \"\"\"\n",
    "    # Preprocess\n",
    "    cleaned_text = preprocessor.clean_text(text)\n",
    "    \n",
    "    # Convert to sequence\n",
    "    sequence = embedding_handler.texts_to_sequences([cleaned_text])\n",
    "    \n",
    "    # Predict\n",
    "    pred = model.predict(sequence, verbose=0)[0]\n",
    "    pred_label = np.argmax(pred)\n",
    "    \n",
    "    print(f\"\\nInput: {text}\")\n",
    "    print(f\"Cleaned: {cleaned_text}\")\n",
    "    print(f\"\\nüéØ Predicted Emotion: {EMOTION_MAP[pred_label]} (confidence: {pred[pred_label]*100:.2f}%)\")\n",
    "    \n",
    "    if show_probabilities:\n",
    "        print(\"\\nüìä All probabilities:\")\n",
    "        for emotion, prob in zip(EMOTION_LABELS, pred):\n",
    "            bar = \"‚ñà\" * int(prob * 50)\n",
    "            print(f\"  {emotion:12s}: {bar} {prob*100:5.2f}%\")\n",
    "\n",
    "print(\"‚úÖ Prediction function created!\")\n",
    "print(\"\\nUsage: predict_emotion('Your text here')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Section 22: Test the Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TESTING PREDICTION FUNCTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test texts\n",
    "test_texts = [\n",
    "    \"I am so happy and excited about this!\",\n",
    "    \"This is really frustrating and makes me angry\",\n",
    "    \"I miss you so much my love\",\n",
    "    \"I am terrified of what might happen\",\n",
    "    \"Oh wow I did not expect that at all!\",\n",
    "    \"I feel so sad and depressed today\"\n",
    "]\n",
    "\n",
    "for test_text in test_texts:\n",
    "    predict_emotion(test_text)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Section 23: Save Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create summary\n",
    "summary = {\n",
    "    'experiment_name': config.experiment_name,\n",
    "    'model_type': config.model_type,\n",
    "    'embedding_type': config.embedding_type,\n",
    "    'embedding_dim': config.embedding_dim,\n",
    "    'rnn_units': config.rnn_units,\n",
    "    'num_layers': config.num_layers,\n",
    "    'dropout': config.dropout,\n",
    "    'final_val_accuracy': float(val_accuracy),\n",
    "    'final_val_loss': float(val_loss),\n",
    "    'best_val_accuracy': float(max(history.history['val_accuracy'])),\n",
    "    'total_epochs_trained': len(history.history['loss']),\n",
    "    'training_samples': len(train_df),\n",
    "    'validation_samples': len(val_df),\n",
    "    'embedding_coverage': stats['coverage_percent'],\n",
    "    'oov_rate_train': float(train_oov),\n",
    "    'oov_rate_val': float(val_oov)\n",
    "}\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nüìã Experiment Summary:\")\n",
    "print(\"=\"*80)\n",
    "for key, value in summary.items():\n",
    "    print(f\"{key:25s}: {value}\")\n",
    "\n",
    "# Save summary\n",
    "summary_file = os.path.join(config.result_dir, f'{config.experiment_name}_summary.json')\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Summary saved to: {summary_file}\")\n",
    "print(f\"‚úÖ Model saved to: {checkpoint_path}\")\n",
    "print(f\"‚úÖ Training log saved to: {csv_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ PIPELINE COMPLETE! üéâ\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüéØ Final Validation Accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"üèÜ Best Validation Accuracy: {max(history.history['val_accuracy'])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Section 24: Quick Experimentation Guide\n",
    "\n",
    "### To Run Different Experiments:\n",
    "\n",
    "1. **Change Model Type**:\n",
    "   ```python\n",
    "   config.model_type = 'gru'  # or 'lstm', 'bilstm'\n",
    "   ```\n",
    "\n",
    "2. **Change Embedding**:\n",
    "   ```python\n",
    "   config.embedding_type = 'word2vec'  # or 'glove'\n",
    "   ```\n",
    "\n",
    "3. **Adjust Model Size**:\n",
    "   ```python\n",
    "   config.rnn_units = 256\n",
    "   config.num_layers = 2\n",
    "   ```\n",
    "\n",
    "4. **Change Training Parameters**:\n",
    "   ```python\n",
    "   config.batch_size = 64\n",
    "   config.learning_rate = 0.0005\n",
    "   config.epochs = 100\n",
    "   ```\n",
    "\n",
    "5. **Then rerun** from Section 8 (Model Creation) onwards!\n",
    "\n",
    "### Files Created:\n",
    "- `saved_models/{experiment_name}_best.keras` - Best model\n",
    "- `logs/{experiment_name}_training.csv` - Training log\n",
    "- `results/{experiment_name}_results.json` - Complete results\n",
    "- `results/{experiment_name}_summary.json` - Quick summary\n",
    "\n",
    "### Next Steps:\n",
    "1. Try different configurations above\n",
    "2. Compare results between experiments\n",
    "3. Test with your own texts using `predict_emotion()`\n",
    "4. Deploy the best model!\n",
    "\n",
    "**This notebook is completely self-contained - all classes and functions are included!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
